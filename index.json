[{"authors":null,"categories":null,"content":"HalleluBERT ‚Äì A Compact Contribution with a State-of-the-Art Impact HalleluBERT was designed as a focused, compact research contribution, a small paper with a clear goal: to build a strong Hebrew RoBERTa-style model and evaluate how far pre-training and scaling would push performance in a low-resource setting.\nAlthough modest in scope, the project delivered something remarkable: the first large Hebrew RoBERTa model trained with a modern pre-training setup, achieving state-of-the-art performance across the evaluation benchmarks we designed.\nThe workflow followed the pattern established in earlier projects.\nThe pre-training was executed on a TPUv4-128 pod, while the entire downstream evaluation was performed locally on private basement hardware, using the same workstation that powered GeistBERT and parts of ChristBERT. In that sense, HalleluBERT represents the final chapter of a long hardware-driven research arc, the last model trained with this infrastructure setup.\nFrom a historical perspective, HalleluBERT is not the most complex or emotionally loaded project in the family. But scientifically, it closes a loop: it rounds off the sequence GottBERT ‚Üí GeistBERT ‚Üí ChristBERT with a clean, technically sharp contribution that stands on its own.\nThe work is publicly available as an arXiv preprint and currently under review for LREC-COLING 2026.\nLinks \u0026amp; Resources Preprint (2025):\nhttps://arxiv.org/abs/2510.21372\nü§ó HuggingFace Model Hub:\nhttps://huggingface.co/HalleluBERT\n","date":1761264e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761264e3,"objectID":"97de34ecd54922fd97120be89330d113","permalink":"/project/hallelubert/","publishdate":"2025-10-24T00:00:00Z","relpermalink":"/project/hallelubert/","section":"project","summary":"A focused Hebrew RoBERTa project delivering the first large Hebrew model with state-of-the-art benchmark performance; pre-trained on a TPUv4-128 pod and evaluated on private hardware.","tags":["NLP","Language Models","Hebrew","Deep Learning","Model Pretraining","Low-Resource NLP","Open Source"],"title":"HalleluBERT","type":"project"},{"authors":null,"categories":null,"content":"SindBert ‚Äì The Turkish Model That Started a New Chapter SindBert started as an exploratory attempt to bring high-quality RoBERTa-style language modeling to Turkish. The idea originally emerged during the broader multilingual phase that followed GottBERT, GeistBERT, ChristBERT, HalleluBERT and PortBERT. After finishing PortBERT, the timing finally felt right to extend this line of research to another underrepresented language.\nWith the pipeline already stable from previous projects, we created the first large-scale RoBERTa-style Turkish model. This makes SindBert a significant technical milestone within the landscape of Turkish transformer models, and an important step toward raising Turkish NLP to the architectural standards used in higher-resource languages.\nThe study behind SindBert was conducted with considerable care. We systematically reviewed existing Turkish models and evaluated them across an extensive set of downstream benchmarks. The evaluation was carried out in two stages: first on a private workstation with two RTX 3090 GPUs in SLI, and later on the LRZ BayernKI H100 cluster, which enabled the training and assessment of the large SindBert variants.\nA key insight from the study was that the performance of Turkish models is not driven simply by corpus size. Extremely large corpora did not consistently translate into stronger results. Instead, the best performance came from models trained on corpora with moderate size but high internal variance and quality. This finding reinforces the idea that thoughtful corpus design often matters more than sheer quantity for languages with complex morphology.\nAlthough SindBERT does not yet have a formal publication attached to it, the project marks the beginning of a new chapter in my international research. It follows directly after PortBERT and represents the point where the broader model family began to expand into new linguistic territory. The work was completed in Freiburg at my new research institution, where the evaluations and analyses were finalized. SindBERT may evolve further or be revisited in a second iteration, but even in its current form it stands as the first large Turkish RoBERTa-style model and an important step toward stronger transformer development for the Turkish language. A preprint of the work is available, and the paper has been submitted to SIGTURK@EACL 2026.\nLinks \u0026amp; Resources Preprint (2025):\nhttps://arxiv.org/abs/2510.21364\nü§ó HuggingFace Model Hub:\nhttps://huggingface.co/SindBERT\n","date":1761264e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1761264e3,"objectID":"242c69e927df29abd1c1f7dbff82207f","permalink":"/project/sindbert/","publishdate":"2025-10-24T00:00:00Z","relpermalink":"/project/sindbert/","section":"project","summary":"The first large Turkish RoBERTa-style model, developed after PortBERT with extensive evaluations on private GPUs and the LRZ BayernKI H100 cluster. The study highlights the importance of corpus variance over sheer size.","tags":["NLP","Language Models","Turkish","Deep Learning","Pretraining","Corpus Design","Open Source","Efficiency","Model Pretraining"],"title":"SindBERT","type":"project"},{"authors":null,"categories":null,"content":"Building a Medical German Language Model Against the Odds ChristBERT was developed as part of my Master‚Äôs thesis and focuses on domain-specific German clinical language modeling. The project investigated two complementary strategies: continued pre-training on top of GottBERT/GeistBERT, and training from scratch using both a general-purpose RoBERTa tokenizer and a specialized medical vocabulary. This enabled a systematic comparison between general, domain-adapted, and fully specialized pre-training pipelines for German medical NLP.\nDuring the early conceptual phase, the supervision structure of the thesis changed, and the project was reassigned to a new advisor, who later became the supervisor of my doctoral work. ChristBERT was trained using a combination of compute resources: through an existing project partnership, parts of the full pre-training were executed on the Augsburg compute cluster, while downstream evaluations, and parts of the translation workflows ran on privately built GPU hardware in my basement.\nChristBERT showed that continued pre-training on German clinical text offers measurable improvements over general-purpose RoBERTa models, while training from scratch with a medical-specific tokenizer provides additional insights into domain vocabulary specialization and its trade-offs.\nThe work has been released as a preprint and is currently under review at BMC Medical Informatics and Decision Making.\nLinks \u0026amp; Resources Preprint (2025):\nhttps://www.researchsquare.com/article/rs-7332811/v1\nü§ó HuggingFace Model Hub:\nhttps://huggingface.co/ChristBERT\n","date":1758153600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1758153600,"objectID":"87f0943f83efca84d4882c2e809d7254","permalink":"/project/christbert/","publishdate":"2025-09-18T00:00:00Z","relpermalink":"/project/christbert/","section":"project","summary":"A domain-adaptive German medical RoBERTa model, exploring continued pre-training and from-scratch training with specialized vocabularies.","tags":["Featured","NLP","Language Models","German","Medical NLP","Deep Learning","Continued Pretraining","Model Pretraining","Domain Adaptation","Open Source","Neural Machine Translation","Corpus Design"],"title":"ChristBERT","type":"project"},{"authors":null,"categories":null,"content":"PortBERT ‚Äì Exploring Efficiency Under Pressure and in Deep Waters PortBERT has its roots in an early collaboration idea from 2021/22, when a Brazilian research group approached me with the vision of building a Portuguese RoBERTa-style model. I shared my pre-training pipeline with them, but at the time, the fairseq RoBERTa branch had a broken implementation and my preprocessing workflow contained an unnoticed bug. As a result, the collaboration could not produce a functional model, and the project quietly came to a halt.\nYears later, during a research stay on the Azores, I revisited the idea independently. The full Portuguese pre-training had actually been carried out earlier, during the GottBERT development phase, when we debugged and stabilized the pipeline. With the system running reliably at that time, I launched a complete Portuguese pre-training run on a TPUv4-128 pod and on a GPU server, resulting in the PortBERT base and large models.\nThe period on the Azores added a unique dimension to the project.\nIt was a time of intense pressure: I was waiting for the contract from my upcoming research position in Freiburg, while simultaneously completing my divemaster internship and training. Much of the evaluation work was carried out between deep dives, decompression lessons, and long days of practical training. This phase taught me, quite literally, to breathe under pressure. In hindsight, PortBERT became the model that was built and evaluated in the rhythm of the ocean: long, calm stretches of computation framed by physical depth and mental discipline.\nThe evaluation revealed an important insight:\nefficiency is an essential dimension of model design.\nPortBERT achieved competitive results despite being smaller and more cost-efficient than many recent Portuguese encoder LLMs. The study also highlighted that the Portuguese corpus could benefit from greater internal variance, and that techniques like Whole Word Masking might offer additional improvements, though WWM was not available within the TPU workflow used here.\nWith this, PortBERT introduced a new perspective into the Portuguese NLP landscape:\nthat efficiency, compute cost, corpus composition, and training dynamics should receive as much attention as raw performance. The goal of the project was not only to build another model, but to encourage the community to look beyond leaderboard scores when developing modern transformer architectures and models.\nPortBERT was presented at GlobalNLP2025@RANLP 2025 in Varna, Bulgaria, and we are currently awaiting its release on the ACL Anthology. The model itself is publicly available on HuggingFace. Links \u0026amp; Resources ü§ó HuggingFace Model Hub:\nhttps://huggingface.co/PortBERT ","date":1757635200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1757635200,"objectID":"01de04cf1f39a3b2c6062650a3382917","permalink":"/project/portbert/","publishdate":"2025-09-12T00:00:00Z","relpermalink":"/project/portbert/","section":"project","summary":"A Portuguese RoBERTa model evaluated during a research stay on the Azores, highlighting efficiency-focused perspective models.","tags":["NLP","Language Models","Portuguese","Deep Learning","Efficiency","Model Pretraining","Open Source"],"title":"PortBERT","type":"project"},{"authors":null,"categories":null,"content":"A Story of Resilience, Constraint, and Scientific Maturity GeistBERT was created during one of the most turbulent and defining phases of my doctoral work. AFollowing a supervisor transition at TUM, access to the compute environment I previously used was no longer available, so I migrated the entire evaluation workflow to my own workstation. What could have halted the project entirely instead became a turning point.\nWith no cluster access, I migrated the full evaluation pipeline into my basement and ran it on a self-built workstation with two RTX 3090 GPUs in SLI. Every downstream task was completed entirely on private hardware. Constraints turned into autonomy, and the work slowly took shape.\nAt that time, the plan was more ambitious:\nwe intended to train Longformer- and Nystr√∂mformer-based variants of GottBERT to compare sparse-attention architectures in continued pre-training on a GPU setup. However, an unnoticed mistake in the pre-training made these large-scale models useless. Instead of releasing partially flawed or inconclusive models, we made the conscious decision to retract the extended architectures and focus the paper on a clean, well-defined contribution.\nThe final version of GeistBERT therefore centered on what was both scientifically solid and practically valuable:\na robust RoBERTa-base model with continued pre-training, using Whole Word Masking and a large, diverse corpus, carefully evaluated across standard German NLP benchmarks.\nDespite being a base model, GeistBERT achieved state-of-the-art performance in multiple tasks and approached, and in some benchmarks even surpassed, the performance of existing large models.\nDuring this phase, GeistBERT also revealed an interesting methodological insight:\nunlike earlier TPU-based experiments, the GPU training workflow allowed for a substantially higher peak learning rate without instability. This raised new research questions about whether the stability originated from implementation differences in the GPU stack, or whether it was a characteristic of continued pre-training itself. These observations gave the project an additional scientific dimension beyond the model alone.\nThe manuscript was completed far from Germany, during a research stay on the Azores, and released as a pre-print. Later, GeistBERT found its academic platform at GlobalNLP2025@RANLP 2025 in Varna, Bulgaria.\nLinks \u0026amp; Resources Preprint (2025):\nhttps://arxiv.org/abs/2506.11903\nü§ó HuggingFace Model Hub:\nhttps://huggingface.co/GeistBERT\n","date":1749772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1749772800,"objectID":"ed29698fc4a3a19b9e8b27f4463eeb29","permalink":"/project/geistbert/","publishdate":"2025-06-13T00:00:00Z","relpermalink":"/project/geistbert/","section":"project","summary":"A continued-pretraining extension of GottBERT, developed during a period of transition and finalized as a preprint before being presented at GlobalNLP@RANLP 2025.","tags":["NLP","Language Models","German","Deep Learning","Continued Pretraining","Model Pretraining","Open Source","Corpus Design"],"title":"GeistBERT","type":"project"},{"authors":null,"categories":null,"content":"From a student project to a polished tool: GPU volume rendering for MATLAB Volume Renderer for use with MATLAB started as my master project at the University of Freiburg in 2012, supervised by Benjamin Ummenhofer and apl. Prof. Dr. Olaf Ronneberger. At the time, MATLAB offered only limited support for interactive, high quality volume rendering of 3D data on the GPU, especially for larger medical volumes and multi volume scenes. The project filled this gap by adding a GPU enabled volume render command to MATLAB that could be used directly from scripts and applications.\nThe core renderer is written in CUDA C and C++, while the user interface is provided through a set of MATLAB classes. This design separates performance critical code from user facing logic and allowed us to integrate advanced rendering concepts into a familiar MATLAB workflow. Users can call the renderer like any other MATLAB function, yet the heavy lifting happens on the GPU.\nA central feature of the system is its custom memory management. GPU memory is limited, but many use cases require rendering multiple large volumes in a single scene. To address this, the renderer splits the scene into separate rendering passes that fit into GPU memory, then combines the resulting images inside MATLAB into a single final frame. On top of that, volumes are only transferred to the GPU when the underlying data changes, which keeps repeated renders fast by reusing GPU memory across calls.\nThe renderer also implements a generic illumination model that can be extended with different phase functions. The provided implementation uses the Henyey Greenstein phase function, which allows realistic lighting effects for volumetric data. For specific applications the tool supports off axis stereo rendering, which makes it possible to create stereo pairs and 3D movies for immersive visualization.\nHigh usability was a design goal from the beginning. The MATLAB interface is built around a small set of classes that make it straightforward to configure scenes, adjust parameters, and generate animations programmatically. This makes the renderer suitable not only for research prototypes, but also for teaching and reproducible figure generation.\nAfter an initial active phase during my studies the project sat idle for several years. When I moved to Munich in 2021 I resumed work on the codebase, updated the toolchain, improved the documentation, and prepared the project for a proper open source release. The result was a journal publication in 2024 and a cleaned up public repository that others can build on.\nVolume Renderer for use with MATLAB is released under the GNU Affero General Public License version 3. The example scripts are licensed under MIT, so they can be easily reused and adapted. The full source code is available on GitHub, together with usage examples and build instructions.\nLinks and Resources Journal Publication (2024):\nhttps://www.mdpi.com/2673-6470/4/4/49\nGitHub Repository:\nhttps://github.com/raphiniert-com/volume_renderer\n","date":1704067200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704067200,"objectID":"c0c82d59f25305347587acd7892e7f77","permalink":"/project/volume-renderer/","publishdate":"2024-01-01T00:00:00Z","relpermalink":"/project/volume-renderer/","section":"project","summary":"A CUDA based GPU volume rendering engine with a MATLAB interface.","tags":["Featured","Other","GPU Rendering","CUDA","MATLAB","Volume Rendering","Medical Imaging","Scientific Visualization","C++","High Performance Computing"],"title":"Volume Renderer for use with MATLAB","type":"project"},{"authors":null,"categories":null,"content":"i2b2RLS ‚Äì Fine-Grained Access Control for Clinical Data Warehouses The idea for i2b2RLS emerged in 2019 when I joined the IMBI team in Freiburg. With my experience from the ESID reporting infrastructure, it became clear that a comparable, policy-driven access-control layer could meaningfully strengthen the security architecture of our local i2b2 installation. The goal was to take the central i2b2 instance at the Freiburg Data Integration Center and deploy it with true row-level access control using PostgreSQL RLS, a level of fine-grained authorization that i2b2 did not support out of the box.\nAchieving this required modifications on two layers: adapting parts of the i2b2 stack itself, and developing a consistent workflow to generate, validate, and deploy PostgreSQL RLS policies. To evaluate feasibility, we created a synthetic dataset and measured RLS performance under realistic i2b2 query patterns. In the course of this work we also conducted, to our knowledge, the first systematic performance benchmark evaluating PostgreSQL Row-Level Security in the context of a clinical research data warehouse. Using the synthetic data and representative query structures, we demonstrated that a carefully designed policy architecture can be deployed in production settings with acceptable overhead.\nTo make the system maintainable and reproducible, we implemented a Python-based deployment tool that automated the generation and installation of RLS policy sets. The tool incorporates pgTAP tests to reduce the risk of deploying faulty or incomplete policies, a critical safeguard for hospital-grade access control. We released the tool as open source on PyPI, making it easy for other institutions to adopt similar security layers.\nFor easier setup, we also created a Docker-based i2b2 stack that includes all required components for RLS deployment, significantly lowering the barrier for testing or adopting this approach in other environments.\nThe project ultimately resulted in a JAMIA Open publication, documenting the architecture, performance evaluation, and practical deployment considerations of applying PostgreSQL Row-Level Security to i2b2 in a clinical data warehouse context.\nLinks \u0026amp; Resources JAMIA Open Publication (2023):\nhttps://academic.oup.com/jamiaopen/article/6/3/ooad068/7242495\nPython Package on PyPI:\nhttps://pypi.org/project/i2b2rls/\nSource Code of all related projects (GitLab Group):\nhttps://gitlab.com/mds-imbi-freiburg/i2b2\n","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672531200,"objectID":"445924f64075660d10323f0a5ef66fa6","permalink":"/project/i2b2rls/","publishdate":"2023-01-01T00:00:00Z","relpermalink":"/project/i2b2rls/","section":"project","summary":"A PostgreSQL Row-Level Security framework for i2b2, including policy generation, automated testing with pgTAP, performance benchmarking, and a Docker-based deployment stack for clinical data warehouses.","tags":["i2b2","Clinical Data Warehouses","Row-Level Security","PostgreSQL","Security","pgTAP","Python","Docker","Data Integration","Medical Informatics"],"title":"i2b2RLS","type":"project"},{"authors":null,"categories":null,"content":"React Admin Data Providers for Clinical APIs ‚Äì PostgREST and FHIR The development of my React Admin data providers began with the MeSH Browser project. At that time, I needed a flexible way to connect a React Admin frontend to a PostgREST backend without manually wiring every query, filter and pagination step. Early community work provided some inspiration, but no complete implementation existed. This led to the first version of the PostgREST data provider, which gradually grew into a fully featured library which is widely used (\u0026gt;2k downloads/week).\nOver time, the project evolved far beyond its original prototype. We added a dedicated test framework, introduced structured configuration, and turned it into a generic ecosystem that makes it easy to build React Admin frontends on top of Postgres. A demo setup was created to showcase multiple PostgreSQL FDWs together with PostgREST, illustrating how flexible the architecture can be when connecting diverse data sources. For authentication and authorization, the system integrates cleanly with Keycloak.\nThe second data provider emerged in a similar way. As part of a student project, we built a FHIR REST data provider for React Admin. Its goal was to simplify building clinical user interfaces directly on top of FHIR servers. The data provider implements FHIR search, pagination, resource handling and bundle interpretation, and was tested against the LinuxForHealth (formerly IBM) FHIR Server. Together with a small demo application, this demonstrated how React Admin can be used as a lightweight tool for building FHIR based administrative interfaces. The project resulted in a peer reviewed conference publication at ICIMTH.\nTogether, these two libraries form a small but valuable toolkit that connects modern React based user interfaces with established clinical APIs. Whether through PostgREST or FHIR, both data providers lower the barrier for creating custom user interfaces in clinical or research environments by offering reliable, clean and reusable integrations that work out of the box.\nLinks \u0026amp; Resources PostgREST Data Provider Journal Publication (Software Impacts, 2024):\nhttps://www.sciencedirect.com/science/article/pii/S2665963824000873\nNPM Package:\nhttps://www.npmjs.com/package/@raphiniert/ra-data-postgrest\nGitHub Repository:\nhttps://github.com/raphiniert-com/ra-data-postgrest\nDemo Repository:\nhttps://github.com/raphiniert-com/ra-data-postgrest-demo\nFHIR Data Provider Conference Publication (ICIMTH 2023):\nhttps://doi.org/10.3233/SHTI230436\nGitLab Repository:\nhttps://gitlab.com/mri-tum/aiim/libs/ra-data-fhir\nNPM Package:\nhttps://www.npmjs.com/package/@tum-mri-aiim/ra-data-fhir\n","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"7f38e546f2685468618f77a4d0164be4","permalink":"/project/ra-data/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/project/ra-data/","section":"project","summary":"A set of reusable React Admin data providers for PostgREST and FHIR, developed to streamline UI development.","tags":["Featured","Medical Informatics","Other","React Admin","PostgREST","FHIR","Clinical APIs","Web Development","Postgres","Keycloak","Open Source"],"title":"React Admin Data Providers","type":"project"},{"authors":null,"categories":null,"content":"Exploring MeSH Through Modern Search: PostgREST, React-Admin, and Elasticsearch The MeSH-Browser originated from a very practical problem: German-speaking editorial teams had no usable interface to explore or apply MeSH terms. The initial motivation came from Cochrane Germany, who needed a tool to support the ‚ÄúWissen Was Wirkt‚Äù blog in assigning German-language tags and categories based on the MeSH vocabulary. At the time, no low-threshold, German-friendly MeSH interface existed. DIMDI provided a German MeSH dataset, but it was difficult to use for anyone without technical expertise, and existing tools were either outdated, incomplete, or too complex for non-technical users. This gap led to the development of the MeSH-Browser.\nThe project was strongly shaped by my earlier experience with API-driven designs from ESID. We built a clean PostgREST API layer on top of a PostgreSQL database and implemented the frontend using React-Admin. To support fast and flexible search, the system integrated PostgreSQL full-text capabilities through ZomboDB and Elasticsearch, enabling instant query responses across the hierarchical MeSH vocabulary. This technical foundation proved exceptionally effective: fast to build, easy to extend, and well suited for structured biomedical terminologies. In fact, the MeSH-Browser was the starting point for the dedicated PostgREST DataProvider project, which grew out of the architectural choices made here.\nTwo highly capable Life-Science informatics students joined the development, helping deliver the browser as part of a focused project module. One of them subsequently completed his bachelor thesis on the system, comparing our browser to the official NLM version. In this small user study, our tool performed better in terms of accessibility and ease of use. This confirmed that the technical choices, PostgREST for the API, React-Admin for the UI, and ZomboDB/Elasticsearch for fast search, enabled a development process that was manageable even for junior developers, while still producing a tool that worked intuitively for non-technical end users.\nThe usability study, published as part of a peer-reviewed contribution, reinforced these findings and even with minimal hardware resources, the browser remained highly performant: the live system still runs smoothly on a VM with only 1 vCPU and 2 GB RAM, demonstrating the efficiency of the underlying architecture.\nThe MeSH-Browser ultimately filled a real and long-standing gap by offering Germany‚Äôs biomedical and health communication community a simple, robust, search-driven, and accessible way to explore MeSH terms ‚Äî something that had never existed in this form before.\nLinks \u0026amp; Resources MeSH-Browser (Live System):\nhttps://mesh-browser.de\nTechnical Publication (2021):\nhttps://ebooks.iospress.nl/doi/10.3233/SHTI210939\nUsability Study (2022):\nhttps://ebooks.iospress.nl/doi/10.3233/SHTI220653\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"905450c5b1978b67975f4e07b8e381e0","permalink":"/project/mesh-browser/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/project/mesh-browser/","section":"project","summary":"A lightweight, API-driven MeSH explorer using PostgREST and React-Admin, offering a simple English and German interface for hierarchical navigation.","tags":["Search and IR","Medical NLP","Ontologies","MeSH","Medical Informatics","React Admin","PostgREST","UX","Data Discovery","Elasticsearch","ZomboDB","Full-Text Search"],"title":"MeSH-Browser","type":"project"},{"authors":null,"categories":null,"content":"A Journey Marked by Endurance, Transition, and Discovery GottBERT started from a simple observation: while BERT-like models were rapidly advancing NLP research and RoBERTa had become a widely adopted standard, there was no single-language RoBERTa model for German. Multilingual models were everywhere, but consistently fell short of high-quality monolingual models in a given language.\nThis gap was the starting point for GottBERT.\nThe first version emerged before my move to Munich, a project driven by scientific curiosity that quickly gained momentum. Trained on a 256-core TPUv3 Pod using the German portion of the OSCAR corpus, the initial GottBERT model followed the original RoBERTa pre-training recipe implemented in fairseq. Even without elaborate hyperparameter tuning, it delivered strong results: across multiple NER and text-classification benchmarks, GottBERT outperformed many tested German and multilingual comparison models.\nWith the relocation to Munich, the second phase of the project began. GottBERT was further refined, more deeply evaluated, and extended with a Large variant. During this phase, we also examined the impact of filtering the OSCAR corpus, a reasonable hypothesis that turned out differently than expected. Despite corpus filtering, performance remained largely unchanged.\nIn the updated EMNLP version, the focus shifted from reporting the initial results to providing a more grounded scientific framing, including a clearer comparison with existing German and multilingual BERT models, an extended evaluation across downstream tasks, and an investigation of how filtering the OSCAR corpus affects pre-training quality.\nToday, GottBERT stands as the first German RoBERTa model, freely available to the research community, originally under AGPLv3, later relicensed under MIT. It remains one of the projects that shaped my research journey: the combination of a clear gap, technical ambition, a geographical transition, academic maturation, and a final result that meaningfully supports the German NLP community.\nLinks \u0026amp; Resources Preprint (2020):\nhttps://arxiv.org/abs/2012.02110v1\nEMNLP 2024 Paper:\nhttps://aclanthology.org/2024.emnlp-main.1183/\nPoster \u0026amp; Presentation (Underline):\nhttps://underline.io/events/469/posters/18793/poster/108142-gottbert-a-pure-german-language-model?tab=abstract\nü§ó HuggingFace Model Hub:\nhttps://huggingface.co/GottBERT\n","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"6b87b0b3197b5297c230bafe2ac042ee","permalink":"/project/gottbert/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/project/gottbert/","section":"project","summary":"The first published German RoBERTa-based model family with a clear development path: from its 2020 preprint to the extended EMNLP 2024 version.","tags":["Featured","NLP","Language Models","German","Deep Learning","Open Source","Model Pretraining"],"title":"GottBERT","type":"project"},{"authors":null,"categories":null,"content":"MIRACUM-Pipe ‚Äì Standardizing NGS Analysis for Precision Oncology MIRACUM-Pipe was one of my main technical contributions during 2019 and 2020, in the context of the MIRACUM consortium and its precision oncology workflows. The pipeline itself had already been established, but required significant modernization, restructuring, and containerization to become robust, reproducible, and easy to deploy across the various MIRACUM sites.\nMy first contribution was a deep refactoring of the existing workflow, which at the time consisted of a set of Bash scripts that had grown organically. I restructured the codebase, cleaned up the scripting logic, modularized core components, and ensured a more maintainable and transparent execution flow. This laid the foundation for the next step: a fully dockerized version of the pipeline that could be executed in a standardized, reproducible way independent of local environments.\nTo achieve this, a second project was created to expose the pipeline via Docker, making installation and usage significantly easier for newly onboarding MIRACUM centers and enabling reproducible runs across different infrastructures. The dockerized version soon became the recommended deployment approach. The entire code was published publicly under the AGPLv3 license.\nAn early attempt was made to publish the pipeline as an Application Note in Bioinformatics, but this submission was not accepted. Later, after several subsequent extensions and improvements, completed when I was no longer actively involved, the pipeline was successfully published in MDPI Cancers. The publication reflects the combined efforts of the team and documents MIRACUM-Pipe as a comprehensive NGS analysis solution.\nMIRACUM-Pipe supports the complete analysis workflow needed in precision oncology and Molecular Tumor Boards (MTBs). It provides a one-prompt solution covering quality control, variant calling, copy number estimation, functional annotation, visualization, and automated report generation. By standardizing these steps, the pipeline supports MTB case preparation and presentation by generating consistent, high-quality sequencing summaries for clinicians and researchers.\nThe associated repositories remain available on GitHub, including both the primary codebase and the dockerized variant, and continue to be used within and beyond the MIRACUM consortium.\nLinks \u0026amp; Resources Publication (Cancers, 2023):\nhttps://www.mdpi.com/2072-6694/15/13/3456\nGitHub Repository:\nhttps://github.com/AG-Boerries/MIRACUM-Pipe\nDockerized Version:\nhttps://github.com/AG-Boerries/MIRACUM-Pipe-docker\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c64321ffb1868d7aa3444123c995a3bb","permalink":"/project/miracum-pipeline/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/miracum-pipeline/","section":"project","summary":"Refactoring and dockerization of the MIRACUM NGS analysis pipeline, enabling standardized, reproducible workflows for precision oncology and Molecular Tumor Boards.","tags":[null,"Bioinformatics","NGS","Docker","Bash","Workflow Engineering","Precision Oncology","Molecular Tumor Boards","Data Pipelines","Reproducibility","Medical Informatics"],"title":"MIRACUM-Pipe","type":"project"},{"authors":null,"categories":null,"content":"tala-med search ‚Äì Engineering a Medical Search Engine from Prototype to Modern IR Platform The tala-med search engine began in 2019 as a GAP funded project. I joined initially as technical support, but after a colleague left the team I took over the development lead and guided the system through several difficult phases into a working prototype that could be evaluated scientifically. The early version of tala-med used a React interface based on design drafts created by a graphic designer in the group. On the backend, we used AppSearch together with the Fess crawler. This setup was sufficient for the first prototype and for the first evaluation, but it became clear that the closed ecosystem limited our flexibility, especially regarding custom NLP components and extensible retrieval pipelines.\nWith this in mind we designed a new technology platform that offered full control over indexing, querying, scoring and synonym handling. The architecture was inspired by the experience gained with the MeSH-Browser and built on top of the subZero stack. As a student project we developed a synonym expansion component using a FastText based approach, and even managed to outperform the previous solution in speed. This work resulted in a journal publication documenting the new retrieval platform and its technical design.\nA second major building block was the crawler. The original Fess setup lacked the flexibility and transparency needed for large scale, domain controlled crawling. We migrated to a custom adapted version of Apache Nutch, which became the basis of a student project that later resulted in a peer reviewed MIE 2025 publication. Together, the new retrieval platform and the improved crawler formed a modern, maintainable and fully open architecture for tala-med.\nBoth contributions were completed during the earlier development phase. I have since continued to work on tala-med, and we are currently preparing the deployment of the new platform to updated servers. The goal is to replace the old prototype implementation in late 2025 or early 2026 with the fully redesigned architecture.\ntala-med is now a fully re-engineered medical search engine with an open, extensible infrastructure, modern crawling, synonym expansion, and a retrieval pipeline that can be adapted to new NLP methods as needed. What started as a constrained prototype has now matured into a flexible research and production environment backed by multiple publications and several years of iterative development.\nLinks \u0026amp; Resources Live Search Instance:\nhttps://suche.tala-med.info\nSource Code (Search Platform Repository):\nhttps://gitlab.com/tala-med/search-platform\nPrototype and Human Factors Evaluation (JMIR Human Factors, 2025):\nhttps://humanfactors.jmir.org/2025/1/e56941/\nTechnical Improvement and Synonym Expansion (Health Informatics Journal, 2025):\nhttps://journals.sagepub.com/doi/full/10.1177/14604582251381271\nCustom Apache Nutch Crawler (MIE 2025):\nhttps://ebooks.iospress.nl/doi/10.3233/SHTI250423\n","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"cd352f56cdeabd3ac468aa29a677ce10","permalink":"/project/tala-med/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/project/tala-med/","section":"project","summary":"A modern medical search engine developed from scratch with a fully open, extensible IR architecture.","tags":["Featured","Search and IR","Medical Informatics","Featured","Medical NLP","Information Retrieval","Medical Search","NLP","FastText","Apache Nutch","SubZero","PostgreSQL","React","Search Engines"],"title":"tala-med Search Engine","type":"project"},{"authors":null,"categories":null,"content":"1. Datenschutz auf einen Blick Allgemeine Hinweise Die folgenden Hinweise geben einen einfachen √úberblick dar√ºber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie pers√∂nlich identifiziert werden k√∂nnen. Ausf√ºhrliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgef√ºhrten Datenschutzerkl√§rung.\nDatenerfassung auf dieser Website Wer ist verantwortlich f√ºr die Datenerfassung auf dieser Website? Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber. Dessen Kontaktdaten k√∂nnen Sie dem Abschnitt ‚ÄûHinweis zur Verantwortlichen Stelle‚Äú in dieser Datenschutzerkl√§rung entnehmen.\nWie erfassen wir Ihre Daten? Ihre Daten werden zum einen dadurch erhoben, dass Sie uns diese mitteilen. Hierbei kann es sich z.¬†B. um Daten handeln, die Sie in ein Kontaktformular eingeben.\nAndere Daten werden automatisch oder nach Ihrer Einwilligung beim Besuch der Website durch unsere IT-Systeme erfasst. Das sind vor allem technische Daten (z.¬†B. Internetbrowser, Betriebssystem oder Uhrzeit des Seitenaufrufs). Die Erfassung dieser Daten erfolgt automatisch, sobald Sie diese Website betreten.\nWof√ºr nutzen wir Ihre Daten? Ein Teil der Daten wird erhoben, um eine fehlerfreie Bereitstellung der Website zu gew√§hrleisten. Andere Daten k√∂nnen zur Analyse Ihres Nutzerverhaltens verwendet werden. Sofern √ºber die Website Vertr√§ge geschlossen oder angebahnt werden k√∂nnen, werden die √ºbermittelten Daten auch f√ºr Vertragsangebote, Bestellungen oder sonstige Auftragsanfragen verarbeitet.\nWelche Rechte haben Sie bez√ºglich Ihrer Daten? Sie haben jederzeit das Recht, unentgeltlich Auskunft √ºber Herkunft, Empf√§nger und Zweck Ihrer gespeicherten personenbezogenen Daten zu erhalten. Sie haben au√üerdem ein Recht, die Berichtigung oder L√∂schung dieser Daten zu verlangen. Wenn Sie eine Einwilligung zur Datenverarbeitung erteilt haben, k√∂nnen Sie diese Einwilligung jederzeit f√ºr die Zukunft widerrufen. Au√üerdem haben Sie das Recht, unter bestimmten Umst√§nden die Einschr√§nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Des Weiteren steht Ihnen ein Beschwerderecht bei der zust√§ndigen Aufsichtsbeh√∂rde zu.\nHierzu sowie zu weiteren Fragen zum Thema Datenschutz k√∂nnen Sie sich jederzeit an uns wenden.\n2. Hosting Wir hosten die Inhalte unserer Website bei folgendem Anbieter:\nExternes Hosting Diese Website wird extern gehostet. Die personenbezogenen Daten, die auf dieser Website erfasst werden, werden auf den Servern des Hosters / der Hoster gespeichert. Hierbei kann es sich v.¬†a. um IP-Adressen, Kontaktanfragen, Meta- und Kommunikationsdaten, Vertragsdaten, Kontaktdaten, Namen, Websitezugriffe und sonstige Daten, die √ºber eine Website generiert werden, handeln.\nDas externe Hosting erfolgt zum Zwecke der Vertragserf√ºllung gegen√ºber unseren potenziellen und bestehenden Kunden (Art. 6 Abs. 1 lit. b DSGVO) und im Interesse einer sicheren, schnellen und effizienten Bereitstellung unseres Online-Angebots durch einen professionellen Anbieter (Art. 6 Abs. 1 lit. f DSGVO). Sofern eine entsprechende Einwilligung abgefragt wurde, erfolgt die Verarbeitung ausschlie√ülich auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO und ¬ß 25 Abs. 1 TDDDG, soweit die Einwilligung die Speicherung von Cookies oder den Zugriff auf Informationen im Endger√§t des Nutzers (z.¬†B. Device-Fingerprinting) im Sinne des TDDDG umfasst. Die Einwilligung ist jederzeit widerrufbar.\nUnser(e) Hoster wird bzw. werden Ihre Daten nur insoweit verarbeiten, wie dies zur Erf√ºllung seiner Leistungspflichten erforderlich ist und unsere Weisungen in Bezug auf diese Daten befolgen.\nWir setzen folgende(n) Hoster ein:\nGitHub Pages\n3. Allgemeine Hinweise und Pflicht¬≠informationen Datenschutz Die Betreiber dieser Seiten nehmen den Schutz Ihrer pers√∂nlichen Daten sehr ernst. Wir behandeln Ihre personenbezogenen Daten vertraulich und entsprechend den gesetzlichen Datenschutzvorschriften sowie dieser Datenschutzerkl√§rung.\nWenn Sie diese Website benutzen, werden verschiedene personenbezogene Daten erhoben. Personenbezogene Daten sind Daten, mit denen Sie pers√∂nlich identifiziert werden k√∂nnen. Die vorliegende Datenschutzerkl√§rung erl√§utert, welche Daten wir erheben und wof√ºr wir sie nutzen. Sie erl√§utert auch, wie und zu welchem Zweck das geschieht.\nWir weisen darauf hin, dass die Daten√ºbertragung im Internet (z.¬†B. bei der Kommunikation per E-Mail) Sicherheitsl√ºcken aufweisen kann. Ein l√ºckenloser Schutz der Daten vor dem Zugriff durch Dritte ist nicht m√∂glich.\nHinweis zur verantwortlichen Stelle Die verantwortliche Stelle f√ºr die Datenverarbeitung auf dieser Website ist:\nRaphael Johannes Scheible-Schmitt\nauftretend unter Raphael Schmitt\nCAYA Postbox 985694\n96035 Bamberg\nDeutschland\nTelefon: +49 1520 826 999 4\nE-Mail: mail@raphiniert.com\nVerantwortliche Stelle ist die nat√ºrliche oder juristische Person, die allein oder gemeinsam mit anderen √ºber die Zwecke und Mittel der Verarbeitung von personenbezogenen Daten (z.¬†B. Namen, E-Mail- Adressen o. √Ñ.) entscheidet.\nSpeicherdauer Soweit innerhalb dieser Datenschutzerkl√§rung keine speziellere Speicherdauer genannt wurde, verbleiben Ihre personenbezogenen Daten bei uns, bis der Zweck f√ºr die Datenverarbeitung entf√§llt. Wenn Sie ein berechtigtes L√∂schersuchen geltend machen oder eine Einwilligung zur Datenverarbeitung widerrufen, werden Ihre Daten gel√∂scht, sofern wir keine anderen rechtlich zul√§ssigen Gr√ºnde f√ºr die Speicherung Ihrer personenbezogenen Daten haben (z.¬†B. steuer- oder handelsrechtliche Aufbewahrungsfristen); im letztgenannten Fall erfolgt die L√∂schung nach Fortfall dieser Gr√ºnde.\nAllgemeine Hinweise zu den Rechtsgrundlagen der Datenverarbeitung auf dieser Website Sofern Sie in die Datenverarbeitung eingewilligt haben, verarbeiten wir Ihre personenbezogenen Daten auf Grundlage von Art. 6 Abs. 1 lit. a DSGVO bzw. Art. 9 Abs. 2 lit. a DSGVO, sofern besondere Datenkategorien nach Art. 9 Abs. 1 DSGVO verarbeitet werden. Im Falle einer ausdr√ºcklichen Einwilligung in die √úbertragung personenbezogener Daten in Drittstaaten erfolgt die Datenverarbeitung au√üerdem auf Grundlage von Art. 49 Abs. 1 lit. a DSGVO. Sofern Sie in die Speicherung von Cookies oder in den Zugriff auf Informationen in Ihr Endger√§t (z.¬†B. via Device-Fingerprinting) eingewilligt haben, erfolgt die Datenverarbeitung zus√§tzlich auf Grundlage von ¬ß 25 Abs. 1 TDDDG. Die Einwilligung ist jederzeit widerrufbar. Sind Ihre Daten zur Vertragserf√ºllung oder zur Durchf√ºhrung vorvertraglicher Ma√ünahmen erforderlich, verarbeiten wir Ihre Daten auf Grundlage des Art. 6 Abs. 1 lit. b DSGVO. Des Weiteren verarbeiten wir Ihre Daten, sofern diese zur Erf√ºllung einer rechtlichen Verpflichtung erforderlich sind auf Grundlage von Art. 6 Abs. 1 lit. c DSGVO. Die Datenverarbeitung kann ferner auf Grundlage unseres berechtigten Interesses nach Art. 6 Abs. 1 lit. f DSGVO erfolgen. √úber die jeweils im Einzelfall einschl√§gigen Rechtsgrundlagen wird in den folgenden Abs√§tzen dieser Datenschutzerkl√§rung informiert.\nEmpf√§nger von personenbezogenen Daten Im Rahmen unserer Gesch√§ftst√§tigkeit arbeiten wir mit verschiedenen externen Stellen zusammen. Dabei ist teilweise auch eine √úbermittlung von personenbezogenen Daten an diese externen Stellen erforderlich. Wir geben personenbezogene Daten nur dann an externe Stellen weiter, wenn dies im Rahmen einer Vertragserf√ºllung erforderlich ist, wenn wir gesetzlich hierzu verpflichtet sind (z.¬†B. Weitergabe von Daten an Steuerbeh√∂rden), wenn wir ein berechtigtes Interesse nach Art. 6 Abs. 1 lit. f DSGVO an der Weitergabe haben oder wenn eine sonstige Rechtsgrundlage die Datenweitergabe erlaubt. Beim Einsatz von Auftragsverarbeitern geben wir personenbezogene Daten unserer Kunden nur auf Grundlage eines g√ºltigen Vertrags √ºber Auftragsverarbeitung weiter. Im Falle einer gemeinsamen Verarbeitung wird ein Vertrag √ºber gemeinsame Verarbeitung geschlossen.\nWiderruf Ihrer Einwilligung zur Datenverarbeitung Viele Datenverarbeitungsvorg√§nge sind nur mit Ihrer ausdr√ºcklichen Einwilligung m√∂glich. Sie k√∂nnen eine bereits erteilte Einwilligung jederzeit widerrufen. Die Rechtm√§√üigkeit der bis zum Widerruf erfolgten Datenverarbeitung bleibt vom Widerruf unber√ºhrt.\nWiderspruchsrecht gegen die Datenerhebung in besonderen F√§llen sowie gegen Direktwerbung (Art. 21 DSGVO) WENN DIE DATENVERARBEITUNG AUF GRUNDLAGE VON ART. 6 ABS. 1 LIT. E ODER F DSGVO ERFOLGT, HABEN SIE JEDERZEIT DAS RECHT, AUS GR√úNDEN, DIE SICH AUS IHRER BESONDEREN SITUATION ERGEBEN, GEGEN DIE VERARBEITUNG IHRER PERSONENBEZOGENEN DATEN WIDERSPRUCH EINZULEGEN; DIES GILT AUCH F√úR EIN AUF DIESE BESTIMMUNGEN GEST√úTZTES PROFILING. DIE JEWEILIGE RECHTSGRUNDLAGE, AUF DENEN EINE VERARBEITUNG BERUHT, ENTNEHMEN SIE DIESER DATENSCHUTZERKL√ÑRUNG. WENN SIE WIDERSPRUCH EINLEGEN, WERDEN WIR IHRE BETROFFENEN PERSONENBEZOGENEN DATEN NICHT MEHR VERARBEITEN, ES SEI DENN, WIR K√ñNNEN ZWINGENDE SCHUTZW√úRDIGE GR√úNDE F√úR DIE VERARBEITUNG NACHWEISEN, DIE IHRE INTERESSEN, RECHTE UND FREIHEITEN √úBERWIEGEN ODER DIE VERARBEITUNG DIENT DER GELTENDMACHUNG, AUS√úBUNG ODER VERTEIDIGUNG VON RECHTSANSPR√úCHEN (WIDERSPRUCH NACH ART. 21 ABS. 1 DSGVO).\nWERDEN IHRE PERSONENBEZOGENEN DATEN VERARBEITET, UM DIREKTWERBUNG ZU BETREIBEN, SO HABEN SIE DAS RECHT, JEDERZEIT WIDERSPRUCH GEGEN DIE VERARBEITUNG SIE BETREFFENDER PERSONENBEZOGENER DATEN ZUM ZWECKE DERARTIGER WERBUNG EINZULEGEN; DIES GILT AUCH F√úR DAS PROFILING, SOWEIT ES MIT SOLCHER DIREKTWERBUNG IN VERBINDUNG STEHT. WENN SIE WIDERSPRECHEN, WERDEN IHRE PERSONENBEZOGENEN DATEN ANSCHLIESSEND NICHT MEHR ZUM ZWECKE DER DIREKTWERBUNG VERWENDET (WIDERSPRUCH NACH ART. 21 ABS. 2 DSGVO).\nBeschwerde¬≠recht bei der zust√§ndigen Aufsichts¬≠beh√∂rde Im Falle von Verst√∂√üen gegen die DSGVO steht den Betroffenen ein Beschwerderecht bei einer Aufsichtsbeh√∂rde, insbesondere in dem Mitgliedstaat ihres gew√∂hnlichen Aufenthalts, ihres Arbeitsplatzes oder des Orts des mutma√ülichen Versto√ües zu. Das Beschwerderecht besteht unbeschadet anderweitiger verwaltungsrechtlicher oder gerichtlicher Rechtsbehelfe.\nRecht auf Daten¬≠√ºbertrag¬≠barkeit Sie haben das Recht, Daten, die wir auf Grundlage Ihrer Einwilligung oder in Erf√ºllung eines Vertrags automatisiert verarbeiten, an sich oder an einen Dritten in einem g√§ngigen, maschinenlesbaren Format aush√§ndigen zu lassen. Sofern Sie die direkte √úbertragung der Daten an einen anderen Verantwortlichen verlangen, erfolgt dies nur, soweit es technisch machbar ist.\nAuskunft, Berichtigung und L√∂schung Sie haben im Rahmen der geltenden gesetzlichen Bestimmungen jederzeit das Recht auf unentgeltliche Auskunft √ºber Ihre gespeicherten personenbezogenen Daten, deren Herkunft und Empf√§nger und den Zweck der Datenverarbeitung und ggf. ein Recht auf Berichtigung oder L√∂schung dieser Daten. Hierzu sowie zu weiteren Fragen zum Thema personenbezogene Daten k√∂nnen Sie sich jederzeit an uns wenden.\nRecht auf Einschr√§nkung der Verarbeitung Sie haben das Recht, die Einschr√§nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Hierzu k√∂nnen Sie sich jederzeit an uns wenden. Das Recht auf Einschr√§nkung der Verarbeitung besteht in folgenden F√§llen:\nWenn Sie die Richtigkeit Ihrer bei uns gespeicherten personenbezogenen Daten bestreiten, ben√∂tigen wir in der Regel Zeit, um dies zu √ºberpr√ºfen. F√ºr die Dauer der Pr√ºfung haben Sie das Recht, die Einschr√§nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn die Verarbeitung Ihrer personenbezogenen Daten unrechtm√§√üig geschah/geschieht, k√∂nnen Sie statt der L√∂schung die Einschr√§nkung der Datenverarbeitung verlangen. Wenn wir Ihre personenbezogenen Daten nicht mehr ben√∂tigen, Sie sie jedoch zur Aus√ºbung, Verteidigung oder Geltendmachung von Rechtsanspr√ºchen ben√∂tigen, haben Sie das Recht, statt der L√∂schung die Einschr√§nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie einen Widerspruch nach Art. 21 Abs. 1 DSGVO eingelegt haben, muss eine Abw√§gung zwischen Ihren und unseren Interessen vorgenommen werden. Solange noch nicht feststeht, wessen Interessen √ºberwiegen, haben Sie das Recht, die Einschr√§nkung der Verarbeitung Ihrer personenbezogenen Daten zu verlangen. Wenn Sie die Verarbeitung Ihrer personenbezogenen Daten eingeschr√§nkt haben, d√ºrfen diese Daten ‚Äì von ihrer Speicherung abgesehen ‚Äì nur mit Ihrer Einwilligung oder zur Geltendmachung, Aus√ºbung oder Verteidigung von Rechtsanspr√ºchen oder zum Schutz der Rechte einer anderen nat√ºrlichen oder juristischen Person oder aus Gr√ºnden eines wichtigen √∂ffentlichen Interesses der Europ√§ischen Union oder eines Mitgliedstaats verarbeitet werden.\nSSL- bzw. TLS-Verschl√ºsselung Diese Seite nutzt aus Sicherheitsgr√ºnden und zum Schutz der √úbertragung vertraulicher Inhalte, wie zum Beispiel Bestellungen oder Anfragen, die Sie an uns als Seitenbetreiber senden, eine SSL- bzw. TLS-Verschl√ºsselung. Eine verschl√ºsselte Verbindung erkennen Sie daran, dass die Adresszeile des Browsers von ‚Äûhttp://‚Äú auf ‚Äûhttps://‚Äú wechselt und an dem Schloss-Symbol in Ihrer Browserzeile.\nWenn die SSL- bzw. TLS-Verschl√ºsselung aktiviert ist, k√∂nnen die Daten, die Sie an uns √ºbermitteln, nicht von Dritten mitgelesen werden.\n4. Datenerfassung auf dieser Website Kontaktformular Wenn Sie uns per Kontaktformular Anfragen zukommen lassen, werden Ihre Angaben aus dem Anfrageformular inklusive der von Ihnen dort angegebenen Kontaktdaten zwecks Bearbeitung der Anfrage und f√ºr den Fall von Anschlussfragen bei uns gespeichert. Diese Daten geben wir nicht ohne Ihre Einwilligung weiter.\nDie Verarbeitung dieser Daten erfolgt auf Grundlage von Art. 6 Abs. 1 lit. b DSGVO, sofern Ihre Anfrage mit der Erf√ºllung eines Vertrags zusammenh√§ngt oder zur Durchf√ºhrung vorvertraglicher Ma√ünahmen erforderlich ist. In allen √ºbrigen F√§llen beruht die Verarbeitung auf unserem berechtigten Interesse an der effektiven Bearbeitung der an uns gerichteten Anfragen (Art. 6 Abs. 1 lit. f DSGVO) oder auf Ihrer Einwilligung (Art. 6 Abs. 1 lit. a DSGVO) sofern diese abgefragt wurde; die Einwilligung ist jederzeit widerrufbar.\nDie von Ihnen im Kontaktformular eingegebenen Daten verbleiben bei uns, bis Sie uns zur L√∂schung auffordern, Ihre Einwilligung zur Speicherung widerrufen oder der Zweck f√ºr die Datenspeicherung entf√§llt (z.¬†B. nach abgeschlossener Bearbeitung Ihrer Anfrage). Zwingende gesetzliche Bestimmungen ‚Äì insbesondere Aufbewahrungsfristen ‚Äì bleiben unber√ºhrt.\nQuelle: https://www.e-recht24.de\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"1. Datenschutz auf einen Blick Allgemeine Hinweise Die folgenden Hinweise geben einen einfachen √úberblick dar√ºber, was mit Ihren personenbezogenen Daten passiert, wenn Sie diese Website besuchen. Personenbezogene Daten sind alle Daten, mit denen Sie pers√∂nlich identifiziert werden k√∂nnen. Ausf√ºhrliche Informationen zum Thema Datenschutz entnehmen Sie unserer unter diesem Text aufgef√ºhrten Datenschutzerkl√§rung.\nDatenerfassung auf dieser Website Wer ist verantwortlich f√ºr die Datenerfassung auf dieser Website? Die Datenverarbeitung auf dieser Website erfolgt durch den Websitebetreiber.","tags":null,"title":"Datenschutzerkl√§rung","type":"page"},{"authors":null,"categories":null,"content":"Angaben gem√§√ü ¬ß 5 TMG Raphael Johannes Scheible-Schmitt\nauftretend unter Raphael Schmitt\nCAYA Postbox 985694\n96035 Bamberg\nDeutschland\nKontakt Telefon: +49 1520 826 999 4\nE- Mail: mail@raphiniert.com\nStreitschlichtung Wir sind nicht bereit oder verpflichtet, an Streitbeilegungsverfahren vor einer Verbraucherschlichtungsstelle teilzunehmen.\nHaftung f√ºr Inhalte Als Diensteanbieter sind wir gem√§√ü ¬ß 7 Abs.1 TMG f√ºr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach ¬ß¬ß 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, √ºbermittelte oder gespeicherte fremde Informationen zu √ºberwachen oder nach Umst√§nden zu forschen, die auf eine rechtswidrige T√§tigkeit hinweisen.\nVerpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unber√ºhrt. Eine diesbez√ºgliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung m√∂glich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\nHaftung f√ºr Links Unser Angebot enth√§lt Links zu externen Websites Dritter, auf deren Inhalte wir keinen Einfluss haben. Deshalb k√∂nnen wir f√ºr diese fremden Inhalte auch keine Gew√§hr √ºbernehmen. F√ºr die Inhalte der verlinkten Seiten ist stets der jeweilige Anbieter oder Betreiber der Seiten verantwortlich. Die verlinkten Seiten wurden zum Zeitpunkt der Verlinkung auf m√∂gliche Rechtsverst√∂√üe √ºberpr√ºft. Rechtswidrige Inhalte waren zum Zeitpunkt der Verlinkung nicht erkennbar.\nEine permanente inhaltliche Kontrolle der verlinkten Seiten ist jedoch ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht zumutbar. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Links umgehend entfernen.\nUrheberrecht Die durch die Seitenbetreiber erstellten Inhalte und Werke auf diesen Seiten unterliegen dem deutschen Urheberrecht. Die Vervielf√§ltigung, Bearbeitung, Verbreitung und jede Art der Verwertung au√üerhalb der Grenzen des Urheberrechtes bed√ºrfen der schriftlichen Zustimmung des jeweiligen Autors bzw. Erstellers. Downloads und Kopien dieser Seite sind nur f√ºr den privaten, nicht kommerziellen Gebrauch gestattet.\nSoweit die Inhalte auf dieser Seite nicht vom Betreiber erstellt wurden, werden die Urheberrechte Dritter beachtet. Insbesondere werden Inhalte Dritter als solche gekennzeichnet. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte umgehend entfernen.\nQuelle: https://www.e-recht24.de/impressum-generator.html\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"Angaben gem√§√ü ¬ß 5 TMG Raphael Johannes Scheible-Schmitt\nauftretend unter Raphael Schmitt\nCAYA Postbox 985694\n96035 Bamberg\nDeutschland\nKontakt Telefon: +49 1520 826 999 4\nE- Mail: mail@raphiniert.com\nStreitschlichtung Wir sind nicht bereit oder verpflichtet, an Streitbeilegungsverfahren vor einer Verbraucherschlichtungsstelle teilzunehmen.\nHaftung f√ºr Inhalte Als Diensteanbieter sind wir gem√§√ü ¬ß 7 Abs.1 TMG f√ºr eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach ¬ß¬ß 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, √ºbermittelte oder gespeicherte fremde Informationen zu √ºberwachen oder nach Umst√§nden zu forschen, die auf eine rechtswidrige T√§tigkeit hinweisen.","tags":null,"title":"Impressum","type":"page"},{"authors":null,"categories":null,"content":"ESID was one of the most formative long-term projects in my early medical informatics career. Through the ESID Registry I had the opportunity to work at the intersection of clinical data quality, registry design, modern database engineering, and distributed reporting systems. It was also the first project where I was able to present work at international conferences early on, and where I could demonstrate empirically that plausibility checks directly lead to higher data quality ‚Äî an insight that later influenced several follow-up systems.\nDuring the early phase of the redesigned ESID registry, the system itself had already been implemented by the core development group. My role focused on publishing the new registry through an Application Note and presenting its concepts and early insights at ESID meetings. In addition, I developed several of the technical components that became central to the operational ecosystem around the registry, most notably the reporting tool, various export mechanisms (including automated email-based exports), and supporting infrastructure for data access and data delivery. Through these contributions I helped make the registry more accessible, more analyzable, and more useful for both clinicians and researchers.\nA major part of my ESID work centered on the reporting and export infrastructure surrounding the registry. While the registry itself was already in place, its operational ecosystem required modernisation. I developed the reporting tool, which unified several existing data flows, including the integration of UKPID registry exports via custom software. As part of this effort, the underlying database was periodically migrated from MySQL to PostgreSQL in an automated pipeline, enabling more robust data handling and laying the foundation for secure, structured access mechanisms such as Row-Level Security. This environment later supported API-based data retrieval, export interfaces, and interactive reporting functions for both internal and external stakeholders. The pipeline was later stabilized and re-implemented as Snakemake workflow.\nWith PostgreSQL in place, we implemented a modern API using the SubZero stack, enabling safe and structured data access. Row-Level Security (RLS) policies were deployed across all tables to guarantee user-specific and registry-specific permissions. In the internal area of the platform, permitted users could generate exports, including specialized outputs such as the APDS study dataset. The system was built around RabbitMQ and Socket-based notifications. The frontend was developed in React and consumed a GraphQL interface built on top of the API layer. The public-facing side provided global registry statistics, while the internal dashboard showed the subset of data relevant to the logged-in user‚Äôs registry assignment.\nLater, there were plans to migrate the entire reporting solution to Apache Superset. Initial groundwork was completed: a connection to the user directory was implemented, and the database was preconfigured with RLS so that dashboards would automatically respect each user‚Äôs data access permissions. However, due to limited resources and the end of the contract period, the full migration was ultimately not carried out.\nThe APDS study was an important use case within the ESID ecosystem. Its dataset required a human-readable export format despite being based on nested data structures. To make these structures visually intuitive, we created a color-coded Excel representation that allowed clinicians to interpret hierarchical data at a glance within a so called sparse spreadsheet. The tool was named Json2Xlsx and was later published in Software Impacts. This export mechanism supported numerous APDS publications and was also used for data exports to industry partners.\nA further component connected to the ESID ecosystem involved the integration of ESID within the German PID-NET infrastructure using the OSSE registry framework. This was documented in a dedicated publication describing how the OSSE bridgehead enables interoperability between the customized ESID registry and the wider network of rare disease registries. While my own work focused primarily on reporting, export mechanisms, API access, and data delivery around ESID, this paper provides the architectural context in which our technical ecosystem operated: it shows how the ESID registry can participate in decentralized, federated queries without relinquishing data sovereignty, and how OSSE acts as an interoperability layer for national and international collaborative research. The approach demonstrated how a highly customized registry like ESID can connect to the OSSE platform via a free and maintainable toolchain, illustrating the broader interoperability landscape in which my ESID-related technical contributions were embedded.\nOverall, ESID represents a central pillar of my early work in registry design, data quality, API-driven architectures, and secure access control via PostgreSQL and RLS. It is one of the projects where methodological development, practical clinical utility, and research contributions came together in a meaningful way, and one that influenced how I approached subsequent registry and data pipeline projects.\nLinks \u0026amp; Resources New ESID Registry ‚Äì Application Note:\nhttps://academic.oup.com/bioinformatics/article/35/24/5367/5526873\nESID Reporting Tool:\nhttps://cci-reporting.uniklinik-freiburg.de/#/\nESID Registry Overview (2020, Frontiers in Immunology):\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC7578818/\nESID Registry Working Party:\nhttps://esid.org/working-parties/registry-working-party/\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"f797f3c1297501a6514ad9f3ad604ae0","permalink":"/project/esid/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/project/esid/","section":"project","summary":"Building the ESID Registry addons with modern reporting, secure APIs, PostgreSQL/RLS, GraphQL/React, and the Json2Xlsx workflow.","tags":["Search and IR","Featured","Clinical Registries","Medical Informatics","Data Quality","PostgreSQL","Row-Level Security","GraphQL","React","API Design","Data Pipelines","JSON2XLSX"],"title":"ESID Registry","type":"project"},{"authors":null,"categories":null,"content":"AL-PID ‚Äì From a Legacy Access Structure to a Functional Clinical Registry AL-PID was the very first project I worked on when I joined the CCI in 2015. What existed initially was a raw Microsoft Access database intended to serve as a registry for patients with ALPS and related primary immunodeficiencies. What was missing was everything required for actual clinical use: a user interface, a coherent data model, plausibility logic, and an export workflow suitable for downstream scientific analysis.\nThe goal was therefore clear: transform a static legacy database into a usable, validated clinical registry.\nI developed a complete user interface in Microsoft Access and Visual Basic, inspired by ESID but built independently. Central to the system was a tree-structured plausibility logic implemented through event-driven hooks such as OnClick and OnChange. This logic ensured consistent, clinically meaningful data entry and protected the dataset from contradictory or incomplete inputs.\nA distinctive aspect of AL-PID was its dynamic, visually guided plausibility system. The UI did not simply validate data in the background, but actively controlled the user‚Äôs workflow by showing or hiding interface elements based on clinical context. Only when certain variables were selected did additional fields, sections, or action buttons appear. This approach allowed clinicians to focus on the exact data relevant to the case at hand, reduced cognitive load, and prevented invalid combinations from ever being entered. It was a lightweight but highly effective form of context-aware data entry entirely implemented within Access and VBA.\nOver time, AL-PID evolved not only at the UI level but also structurally. The original data model had grown organically and lacked clear boundaries between clinical entities. I refactored and abstracted the schema: redundant fields were consolidated, variable groups reorganized, and a clean separation between MDAT and IDAT was introduced to satisfy data protection requirements. The result was a modular and extensible data model that improved maintainability, validation, and long-term data quality. This evolving structure became the backbone that enabled stable exports for subsequent statistical workflows.\nDuring my later guest scientist period at the CCI, clinicians required a publication-ready export. We designed an Excel-based format that medical staff could populate without technical hurdles. I then implemented a Python-based exporter that converted the sheets into a standardized dataset suitable for further analysis. This workflow ultimately enabled a successful publication in The Lancet Haematology, marking both the scientific impact of the project and the maturity of the registry system.\nLooking back, AL-PID was a defining early project for me: a demanding but formative introduction to clinical registries, data modeling, and the practical realities of medical informatics. Many of the principles developed here later influenced my subsequent registry and data pipeline work, including ESID and OSSE-based systems.\nLinks \u0026amp; Resources Project Overview (CCI Freiburg):\nhttps://www.uniklinik-freiburg.de/cci/studien/alpsal-pid.html\nü§ó Publication (The Lancet Haematology):\nhttps://www.thelancet.com/journals/lanhae/article/PIIS2352-3026(23)00362-9\n","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"f09c42bcebb45a2951b6bf802076dfbb","permalink":"/project/al-pid/","publishdate":"2016-01-01T00:00:00Z","relpermalink":"/project/al-pid/","section":"project","summary":"A clinically driven registry rebuilt from a legacy Access database with a custom UI, modular plausibility logic, and an export pipeline.","tags":["Clinical Registries","Medical Informatics","Access/VBA","Data Quality","Plausibility Checks","MDAT/IDAT Separation","Python"],"title":"AL-PID Registry","type":"project"},{"authors":null,"categories":null,"content":"Efficient octree traversal for robot navigation with OctoMap My first larger C++ project began in 2010 when I wrote my bachelor thesis in the context of OctoMap, the well known open source framework for probabilistic 3D mapping. The project marked my first real contact with C++ and introduced me to volumetric data structures, robotics concepts, and efficient tree based algorithms.\nThe goal of the thesis was to investigate and accelerate octree traversal methods for ray based operations in 3D volumetric maps. Autonomous robots rely on probabilistic 3D models for collision free navigation and path planning, and OctoMap provides such models by storing occupancy information in a compressed octree structure. Insertions and updates are driven by raycasting and raytracing, which in turn depend heavily on traversal performance.\nThe work explored multiple traversal strategies within OctoMap and evaluated how shortcuts, reorganized traversal orderings, and memory handling affect the speed of ray based operations. The experiments demonstrated that raytracing can be accelerated through specific traversal shortcuts, while raycasting benefits significantly from improved memory management strategies. These optimizations showed measurable gains in the insertion and update cycle of 3D sensor data.\nBuilding on these findings, the thesis also introduced an approach for segmenting OctoMaps into connected components of free or occupied space. Two segmentation variants were implemented and analyzed, one optimized for the compressed octree representation, and one for the expanded representation. The resulting algorithms were statistically evaluated and demonstrated how connected components could form a foundation for later object recognition or higher level map analysis.\nThis project was my entry point into robotics oriented software engineering and 3D data structures, and it remains a meaningful milestone from the early days of my academic path.\nLinks and Resources OctoMap Project Homepage:\nhttps://octomap.github.io/ ","date":1292803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1292803200,"objectID":"d16699bcc9593f055b20ee478f112bf3","permalink":"/project/octomap/","publishdate":"2010-12-20T00:00:00Z","relpermalink":"/project/octomap/","section":"project","summary":"Efficient octree traversal and ray based operations for 3D robotic mapping, developed as part of my bachelor thesis.","tags":["Other","OctoMap","Robotics","3D Mapping","C++","Octrees","Raycasting","Raytracing","Occupancy Grids","Connected Components"],"title":"OctoMap Contribution","type":"project"},{"authors":null,"categories":null,"content":"Porzellanland Schmitt GmbH ‚Äì A Family Business Shaped by Early Digital Entrepreneurship My involvement with Porzellanland began long before my academic career. As a highschool student, I accompanied my parents when they started a porcelain trading business. The seed for the business was planted in a simple everyday situation: my grandmother needed a single replacement cup from a discontinued porcelain series. My mother complained that sellers only offered complete sets. I spontaneously suggested, \u0026ldquo;Then buy the whole set, gift the parts you need, and sell the rest with profit.\u0026rdquo; That idea became the initial spark for what later grew into a full busieness.\nVery early, I became responsible for the technical backbone of the business. I set up the first online shop using osCommerce, including server installation, configuration, and the complete infrastructure needed for a functioning e-commerce platform. The limitations of osCommerce, combined with the flexibility of its open source structure and documentation, motivated me to build a fully custom shop system as part of my Abitur project. This system ran successfully for several years and formed the operational foundation of the business during its early growth.\nDuring my master\u0026rsquo;s studies, I migrated the shop to Magento Open Source and developed custom modules, including region-based shipping calculations and a LaTeX based PDF invoice generator. Later, the business transitioned to Shopify, where the shop continued to evolve without my direct involvement.\nBeyond the technical aspects, I also contributed to shaping the brand identity. The name Porzellanland, the domain, and the original logo were all created in collaboration with me. Over the years, the company became one of the largest specialized dealers for discontinued branded porcelain, shipping internationally and serving a niche market with high reliability.\nToday, the business is in its final chapter and planned to be dissolved at the end of the year 2025. Yet Porzellanland remains an important part of my personal and entrepreneurial story, a place where I learned early how technology, design, and business can work together, long before I entered the academic world.\nLinks \u0026amp; Resources Porzellanland Website:\nhttps://porzellanland.de ","date":1104537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1104537600,"objectID":"c360e1b65adaa5427597411d2d68a0f5","permalink":"/project/porzellanland/","publishdate":"2005-01-01T00:00:00Z","relpermalink":"/project/porzellanland/","section":"project","summary":"A family business that shaped my early digital and entrepreneurial skills.","tags":["Other","Entrepreneurship","E-Commerce","Web Development","Magento","Shopify","osCommerce","LaTeX","Branding","Family Business"],"title":"Porzellanland Schmitt GmbH","type":"project"}]