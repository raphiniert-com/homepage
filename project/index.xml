<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</title><link>/project/</link><description>Recent content in Projects on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year}</copyright><lastBuildDate>Fri, 24 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="/project/index.xml" rel="self" type="application/rss+xml"/><item><title>HalleluBERT</title><link>/project/hallelubert/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>/project/hallelubert/</guid><description>&lt;h2 id="hallelubert--a-compact-contribution-with-a-state-of-the-art-impact">HalleluBERT â€“ A Compact Contribution with a State-of-the-Art Impact&lt;/h2>
&lt;p>HalleluBERT was designed as a focused, compact research contribution, a small paper with a clear goal: to build a strong Hebrew RoBERTa-style model and evaluate how far pre-training and scaling would push performance in a low-resource setting.&lt;/p>
&lt;p>Although modest in scope, the project delivered something remarkable: the first large Hebrew RoBERTa model trained with a modern pre-training setup, achieving state-of-the-art performance across the evaluation benchmarks we designed.&lt;/p>
&lt;p>The workflow followed the pattern established in earlier projects.&lt;br>
The pre-training was executed on a TPUv4-128 pod, while the entire downstream evaluation was performed locally on private basement hardware, using the same workstation that powered GeistBERT and parts of ChristBERT. In that sense, HalleluBERT represents the final chapter of a long hardware-driven research arc, the last model trained with this infrastructure setup.&lt;/p>
&lt;p>From a historical perspective, HalleluBERT is not the most complex or emotionally loaded project in the family. But scientifically, it closes a loop: it rounds off the sequence GottBERT â†’ GeistBERT â†’ ChristBERT with a clean, technically sharp contribution that stands on its own.&lt;/p>
&lt;p>The work is publicly available as an arXiv preprint and currently under review for LREC-COLING 2026.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2510.21372">https://arxiv.org/abs/2510.21372&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/HalleluBERT">https://huggingface.co/HalleluBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>SindBERT</title><link>/project/sindbert/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>/project/sindbert/</guid><description>&lt;h2 id="sindbert--the-turkish-model-that-started-a-new-chapter">SindBert â€“ The Turkish Model That Started a New Chapter&lt;/h2>
&lt;p>SindBert started as an exploratory attempt to bring high-quality RoBERTa-style language modeling to Turkish. The idea originally emerged during the broader multilingual phase that followed GottBERT, GeistBERT, ChristBERT, HalleluBERT and PortBERT. After finishing PortBERT, the timing finally felt right to extend this line of research to another underrepresented language.&lt;/p>
&lt;p>With the pipeline already stable from previous projects, we created the first large-scale RoBERTa-style Turkish model. This makes SindBert a significant technical milestone within the landscape of Turkish transformer models, and an important step toward raising Turkish NLP to the architectural standards used in higher-resource languages.&lt;/p>
&lt;p>The study behind SindBert was conducted with considerable care. We systematically reviewed existing Turkish models and evaluated them across an extensive set of downstream benchmarks. The evaluation was carried out in two stages: first on a private workstation with two RTX 3090 GPUs in SLI, and later on the LRZ BayernKI H100 cluster, which enabled the training and assessment of the large SindBert variants.&lt;/p>
&lt;p>A key insight from the study was that the performance of Turkish models is not driven simply by corpus size. Extremely large corpora did not consistently translate into stronger results. Instead, the best performance came from models trained on corpora with moderate size but high internal variance and quality. This finding reinforces the idea that thoughtful corpus design often matters more than sheer quantity for languages with complex morphology.&lt;/p>
&lt;p>Although SindBERT does not yet have a formal publication attached to it, the project marks the beginning of a new chapter in my international research. It follows directly after PortBERT and represents the point where the broader model family began to expand into new linguistic territory. The work was completed in Freiburg at my new research institution, where the evaluations and analyses were finalized. SindBERT may evolve further or be revisited in a second iteration, but even in its current form it stands as the first large Turkish RoBERTa-style model and an important step toward stronger transformer development for the Turkish language. A preprint of the work is available, and the paper has been submitted to SIGTURK@EACL 2026.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2510.21364">https://arxiv.org/abs/2510.21364&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/SindBERT">https://huggingface.co/SindBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>ChristBERT</title><link>/project/christbert/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>/project/christbert/</guid><description>&lt;h2 id="building-a-medical-german-language-model-against-the-odds">Building a Medical German Language Model Against the Odds&lt;/h2>
&lt;p>ChristBERT was developed as part of my Masterâ€™s thesis and focuses on domain-specific German clinical language modeling. The project investigated two complementary strategies: continued pre-training on top of GottBERT/GeistBERT, and training from scratch using both a general-purpose RoBERTa tokenizer and a specialized medical vocabulary. This enabled a systematic comparison between general, domain-adapted, and fully specialized pre-training pipelines for German medical NLP.&lt;/p>
&lt;p>During the early conceptual phase, the supervision structure of the thesis changed, and the project was reassigned to a new advisor, who later became the supervisor of my doctoral work. ChristBERT was trained using a combination of compute resources: through an existing project partnership, parts of the full pre-training were executed on the Augsburg compute cluster, while downstream evaluations, and parts of the translation workflows ran on privately built GPU hardware in my basement.&lt;/p>
&lt;p>ChristBERT showed that continued pre-training on German clinical text offers measurable improvements over general-purpose RoBERTa models, while training from scratch with a medical-specific tokenizer provides additional insights into domain vocabulary specialization and its trade-offs.&lt;/p>
&lt;p>The work has been released as a preprint and is currently under review at BMC Medical Informatics and Decision Making.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://www.researchsquare.com/article/rs-7332811/v1">https://www.researchsquare.com/article/rs-7332811/v1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/ChristBERT">https://huggingface.co/ChristBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>PortBERT</title><link>/project/portbert/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>/project/portbert/</guid><description>&lt;h2 id="portbert--exploring-efficiency-under-pressure-and-in-deep-waters">PortBERT â€“ Exploring Efficiency Under Pressure and in Deep Waters&lt;/h2>
&lt;p>PortBERT has its roots in an early collaboration idea from 2021/22, when a Brazilian research group approached me with the vision of building a Portuguese RoBERTa-style model. I shared my pre-training pipeline with them, but at the time, the fairseq RoBERTa branch had a broken implementation and my preprocessing workflow contained an unnoticed bug. As a result, the collaboration could not produce a functional model, and the project quietly came to a halt.&lt;/p>
&lt;p>Years later, during a research stay on the Azores, I revisited the idea independently. The full Portuguese pre-training had actually been carried out earlier, during the GottBERT development phase, when we debugged and stabilized the pipeline. With the system running reliably at that time, I launched a complete Portuguese pre-training run on a TPUv4-128 pod and on a GPU server, resulting in the PortBERT base and large models.&lt;/p>
&lt;p>The period on the Azores added a unique dimension to the project.&lt;br>
It was a time of intense pressure: I was waiting for the contract from my upcoming research position in Freiburg, while simultaneously completing my divemaster internship and training. Much of the evaluation work was carried out between deep dives, decompression lessons, and long days of practical training. This phase taught me, quite literally, to breathe under pressure. In hindsight, PortBERT became the model that was built and evaluated in the rhythm of the ocean: long, calm stretches of computation framed by physical depth and mental discipline.&lt;/p>
&lt;p>The evaluation revealed an important insight:&lt;br>
efficiency is an essential dimension of model design.&lt;br>
PortBERT achieved competitive results despite being smaller and more cost-efficient than many recent Portuguese encoder LLMs. The study also highlighted that the Portuguese corpus could benefit from greater internal variance, and that techniques like Whole Word Masking might offer additional improvements, though WWM was not available within the TPU workflow used here.&lt;/p>
&lt;p>With this, PortBERT introduced a new perspective into the Portuguese NLP landscape:&lt;br>
that efficiency, compute cost, corpus composition, and training dynamics should receive as much attention as raw performance. The goal of the project was not only to build another model, but to encourage the community to look beyond leaderboard scores when developing modern transformer architectures and models.&lt;/p>
&lt;h2 id="portbert-was-presented-at-globalnlp2025ranlp-2025httpsglobalnlp2025githubio-in-varna-bulgaria-and-we-are-currently-awaiting-its-release-on-the-acl-anthology-the-model-itself-is-publicly-available-on-huggingface">PortBERT was presented at &lt;a href="https://globalnlp2025.github.io">GlobalNLP2025@RANLP 2025&lt;/a> in Varna, Bulgaria, and we are currently awaiting its release on the ACL Anthology. The model itself is publicly available on HuggingFace.&lt;/h2>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/PortBERT">https://huggingface.co/PortBERT&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>GeistBERT</title><link>/project/geistbert/</link><pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate><guid>/project/geistbert/</guid><description>&lt;h2 id="a-story-of-resilience-constraint-and-scientific-maturity">A Story of Resilience, Constraint, and Scientific Maturity&lt;/h2>
&lt;p>GeistBERT was created during one of the most turbulent and defining phases of my doctoral work. AFollowing a supervisor transition at TUM, access to the compute environment I previously used was no longer available, so I migrated the entire evaluation workflow to my own workstation. What could have halted the project entirely instead became a turning point.&lt;/p>
&lt;p>With no cluster access, I migrated the full evaluation pipeline into my basement and ran it on a self-built workstation with two RTX 3090 GPUs in SLI. Every downstream task was completed entirely on private hardware. Constraints turned into autonomy, and the work slowly took shape.&lt;/p>
&lt;p>At that time, the plan was more ambitious:&lt;br>
we intended to train Longformer- and NystrÃ¶mformer-based variants of GottBERT to compare sparse-attention architectures in continued pre-training on a GPU setup. However, an unnoticed mistake in the pre-training made these large-scale models useless. Instead of releasing partially flawed or inconclusive models, we made the conscious decision to retract the extended architectures and focus the paper on a clean, well-defined contribution.&lt;/p>
&lt;p>The final version of GeistBERT therefore centered on what was both scientifically solid and practically valuable:&lt;br>
a robust RoBERTa-base model with continued pre-training, using Whole Word Masking and a large, diverse corpus, carefully evaluated across standard German NLP benchmarks.&lt;br>
Despite being a base model, GeistBERT achieved state-of-the-art performance in multiple tasks and approached, and in some benchmarks even surpassed, the performance of existing large models.&lt;/p>
&lt;p>During this phase, GeistBERT also revealed an interesting methodological insight:&lt;br>
unlike earlier TPU-based experiments, the GPU training workflow allowed for a substantially higher peak learning rate without instability. This raised new research questions about whether the stability originated from implementation differences in the GPU stack, or whether it was a characteristic of continued pre-training itself. These observations gave the project an additional scientific dimension beyond the model alone.&lt;/p>
&lt;p>The manuscript was completed far from Germany, during a research stay on the Azores, and released as a pre-print. Later, GeistBERT found its academic platform at &lt;a href="https://globalnlp2025.github.io">GlobalNLP2025@RANLP 2025&lt;/a> in Varna, Bulgaria.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2506.11903">https://arxiv.org/abs/2506.11903&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/GeistBERT">https://huggingface.co/GeistBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Volume Renderer for use with MATLAB</title><link>/project/volume-renderer/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>/project/volume-renderer/</guid><description>&lt;h2 id="from-a-student-project-to-a-polished-tool-gpu-volume-rendering-for-matlab">From a student project to a polished tool: GPU volume rendering for MATLAB&lt;/h2>
&lt;p>Volume Renderer for use with MATLAB started as my master project at the University of Freiburg in 2012, supervised by Benjamin Ummenhofer and apl. Prof. Dr. Olaf Ronneberger. At the time, MATLAB offered only limited support for interactive, high quality volume rendering of 3D data on the GPU, especially for larger medical volumes and multi volume scenes. The project filled this gap by adding a GPU enabled volume render command to MATLAB that could be used directly from scripts and applications.&lt;/p>
&lt;p>The core renderer is written in CUDA C and C++, while the user interface is provided through a set of MATLAB classes. This design separates performance critical code from user facing logic and allowed us to integrate advanced rendering concepts into a familiar MATLAB workflow. Users can call the renderer like any other MATLAB function, yet the heavy lifting happens on the GPU.&lt;/p>
&lt;p>A central feature of the system is its custom memory management. GPU memory is limited, but many use cases require rendering multiple large volumes in a single scene. To address this, the renderer splits the scene into separate rendering passes that fit into GPU memory, then combines the resulting images inside MATLAB into a single final frame. On top of that, volumes are only transferred to the GPU when the underlying data changes, which keeps repeated renders fast by reusing GPU memory across calls.&lt;/p>
&lt;p>The renderer also implements a generic illumination model that can be extended with different phase functions. The provided implementation uses the Henyey Greenstein phase function, which allows realistic lighting effects for volumetric data. For specific applications the tool supports off axis stereo rendering, which makes it possible to create stereo pairs and 3D movies for immersive visualization.&lt;/p>
&lt;p>High usability was a design goal from the beginning. The MATLAB interface is built around a small set of classes that make it straightforward to configure scenes, adjust parameters, and generate animations programmatically. This makes the renderer suitable not only for research prototypes, but also for teaching and reproducible figure generation.&lt;/p>
&lt;p>After an initial active phase during my studies the project sat idle for several years. When I moved to Munich in 2021 I resumed work on the codebase, updated the toolchain, improved the documentation, and prepared the project for a proper open source release. The result was a journal publication in 2024 and a cleaned up public repository that others can build on.&lt;/p>
&lt;p>Volume Renderer for use with MATLAB is released under the GNU Affero General Public License version 3. The example scripts are licensed under MIT, so they can be easily reused and adapted. The full source code is available on GitHub, together with usage examples and build instructions.&lt;/p>
&lt;hr>
&lt;h2 id="links-and-resources">Links and Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Journal Publication (2024):&lt;/strong>&lt;br>
&lt;a href="https://www.mdpi.com/2673-6470/4/4/49">https://www.mdpi.com/2673-6470/4/4/49&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitHub Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/volume_renderer">https://github.com/raphiniert-com/volume_renderer&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>i2b2RLS</title><link>/project/i2b2rls/</link><pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate><guid>/project/i2b2rls/</guid><description>&lt;h2 id="i2b2rls--fine-grained-access-control-for-clinical-data-warehouses">i2b2RLS â€“ Fine-Grained Access Control for Clinical Data Warehouses&lt;/h2>
&lt;p>The idea for i2b2RLS emerged in 2019 when I joined the IMBI team in Freiburg. With my experience from the ESID reporting infrastructure, it became clear that a comparable, policy-driven access-control layer could meaningfully strengthen the security architecture of our local i2b2 installation. The goal was to take the central i2b2 instance at the Freiburg Data Integration Center and deploy it with true row-level access control using PostgreSQL RLS, a level of fine-grained authorization that i2b2 did not support out of the box.&lt;/p>
&lt;p>Achieving this required modifications on two layers: adapting parts of the i2b2 stack itself, and developing a consistent workflow to generate, validate, and deploy PostgreSQL RLS policies. To evaluate feasibility, we created a synthetic dataset and measured RLS performance under realistic i2b2 query patterns. In the course of this work we also conducted, to our knowledge, the first systematic performance benchmark evaluating PostgreSQL Row-Level Security in the context of a clinical research data warehouse. Using the synthetic data and representative query structures, we demonstrated that a carefully designed policy architecture can be deployed in production settings with acceptable overhead.&lt;/p>
&lt;p>To make the system maintainable and reproducible, we implemented a Python-based deployment tool that automated the generation and installation of RLS policy sets. The tool incorporates pgTAP tests to reduce the risk of deploying faulty or incomplete policies, a critical safeguard for hospital-grade access control. We released the tool as open source on PyPI, making it easy for other institutions to adopt similar security layers.&lt;/p>
&lt;p>For easier setup, we also created a Docker-based i2b2 stack that includes all required components for RLS deployment, significantly lowering the barrier for testing or adopting this approach in other environments.&lt;/p>
&lt;p>The project ultimately resulted in a JAMIA Open publication, documenting the architecture, performance evaluation, and practical deployment considerations of applying PostgreSQL Row-Level Security to i2b2 in a clinical data warehouse context.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>JAMIA Open Publication (2023):&lt;/strong>&lt;br>
&lt;a href="https://academic.oup.com/jamiaopen/article/6/3/ooad068/7242495">https://academic.oup.com/jamiaopen/article/6/3/ooad068/7242495&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Python Package on PyPI:&lt;/strong>&lt;br>
&lt;a href="https://pypi.org/project/i2b2rls/">https://pypi.org/project/i2b2rls/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Source Code of all related projects (GitLab Group):&lt;/strong>&lt;br>
&lt;a href="https://gitlab.com/mds-imbi-freiburg/i2b2">https://gitlab.com/mds-imbi-freiburg/i2b2&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>React Admin Data Providers</title><link>/project/ra-data/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>/project/ra-data/</guid><description>&lt;h2 id="react-admin-data-providers-for-clinical-apis--postgrest-and-fhir">React Admin Data Providers for Clinical APIs â€“ PostgREST and FHIR&lt;/h2>
&lt;p>The development of my React Admin data providers began with the MeSH Browser project. At that time, I needed a flexible way to connect a React Admin frontend to a PostgREST backend without manually wiring every query, filter and pagination step. Early community work provided some inspiration, but no complete implementation existed. This led to the first version of the PostgREST data provider, which gradually grew into a fully featured library which is widely used (&amp;gt;2k downloads/week).&lt;/p>
&lt;p>Over time, the project evolved far beyond its original prototype. We added a dedicated test framework, introduced structured configuration, and turned it into a generic ecosystem that makes it easy to build React Admin frontends on top of Postgres. A demo setup was created to showcase multiple PostgreSQL FDWs together with PostgREST, illustrating how flexible the architecture can be when connecting diverse data sources. For authentication and authorization, the system integrates cleanly with Keycloak.&lt;/p>
&lt;p>The second data provider emerged in a similar way. As part of a student project, we built a FHIR REST data provider for React Admin. Its goal was to simplify building clinical user interfaces directly on top of FHIR servers. The data provider implements FHIR search, pagination, resource handling and bundle interpretation, and was tested against the LinuxForHealth (formerly IBM) FHIR Server. Together with a small demo application, this demonstrated how React Admin can be used as a lightweight tool for building FHIR based administrative interfaces. The project resulted in a peer reviewed conference publication at ICIMTH.&lt;/p>
&lt;p>Together, these two libraries form a small but valuable toolkit that connects modern React based user interfaces with established clinical APIs. Whether through PostgREST or FHIR, both data providers lower the barrier for creating custom user interfaces in clinical or research environments by offering reliable, clean and reusable integrations that work out of the box.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;h3 id="postgrest-data-provider">PostgREST Data Provider&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Journal Publication (Software Impacts, 2024):&lt;/strong>&lt;br>
&lt;a href="https://www.sciencedirect.com/science/article/pii/S2665963824000873">https://www.sciencedirect.com/science/article/pii/S2665963824000873&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NPM Package:&lt;/strong>&lt;br>
&lt;a href="https://www.npmjs.com/package/@raphiniert/ra-data-postgrest">https://www.npmjs.com/package/@raphiniert/ra-data-postgrest&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitHub Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/ra-data-postgrest">https://github.com/raphiniert-com/ra-data-postgrest&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Demo Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/ra-data-postgrest-demo">https://github.com/raphiniert-com/ra-data-postgrest-demo&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="fhir-data-provider">FHIR Data Provider&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Conference Publication (ICIMTH 2023):&lt;/strong>&lt;br>
&lt;a href="https://doi.org/10.3233/SHTI230436">https://doi.org/10.3233/SHTI230436&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitLab Repository:&lt;/strong>&lt;br>
&lt;a href="https://gitlab.com/mri-tum/aiim/libs/ra-data-fhir">https://gitlab.com/mri-tum/aiim/libs/ra-data-fhir&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NPM Package:&lt;/strong>&lt;br>
&lt;a href="https://www.npmjs.com/package/@tum-mri-aiim/ra-data-fhir">https://www.npmjs.com/package/@tum-mri-aiim/ra-data-fhir&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>MeSH-Browser</title><link>/project/mesh-browser/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>/project/mesh-browser/</guid><description>&lt;h2 id="exploring-mesh-through-modern-search-postgrest-react-admin-and-elasticsearch">Exploring MeSH Through Modern Search: PostgREST, React-Admin, and Elasticsearch&lt;/h2>
&lt;p>The MeSH-Browser originated from a very practical problem: German-speaking editorial teams had no usable interface to explore or apply MeSH terms. The initial motivation came from Cochrane Germany, who needed a tool to support the â€œWissen Was Wirktâ€ blog in assigning German-language tags and categories based on the MeSH vocabulary. At the time, no low-threshold, German-friendly MeSH interface existed. DIMDI provided a German MeSH dataset, but it was difficult to use for anyone without technical expertise, and existing tools were either outdated, incomplete, or too complex for non-technical users. This gap led to the development of the MeSH-Browser.&lt;/p>
&lt;p>The project was strongly shaped by my earlier experience with API-driven designs from ESID. We built a clean PostgREST API layer on top of a PostgreSQL database and implemented the frontend using React-Admin. To support fast and flexible search, the system integrated PostgreSQL full-text capabilities through ZomboDB and Elasticsearch, enabling instant query responses across the hierarchical MeSH vocabulary. This technical foundation proved exceptionally effective: fast to build, easy to extend, and well suited for structured biomedical terminologies. In fact, the MeSH-Browser was the starting point for the dedicated PostgREST DataProvider project, which grew out of the architectural choices made here.&lt;/p>
&lt;p>Two highly capable Life-Science informatics students joined the development, helping deliver the browser as part of a focused project module. One of them subsequently completed his bachelor thesis on the system, comparing our browser to the official NLM version. In this small user study, our tool performed better in terms of accessibility and ease of use. This confirmed that the technical choices, PostgREST for the API, React-Admin for the UI, and ZomboDB/Elasticsearch for fast search, enabled a development process that was manageable even for junior developers, while still producing a tool that worked intuitively for non-technical end users.&lt;/p>
&lt;p>The usability study, published as part of a peer-reviewed contribution, reinforced these findings and even with minimal hardware resources, the browser remained highly performant: the live system still runs smoothly on a VM with only 1 vCPU and 2 GB RAM, demonstrating the efficiency of the underlying architecture.&lt;/p>
&lt;p>The MeSH-Browser ultimately filled a real and long-standing gap by offering Germanyâ€™s biomedical and health communication community a simple, robust, search-driven, and accessible way to explore MeSH terms â€” something that had never existed in this form before.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>MeSH-Browser (Live System):&lt;/strong>&lt;br>
&lt;a href="https://mesh-browser.de">https://mesh-browser.de&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Technical Publication (2021):&lt;/strong>&lt;br>
&lt;a href="https://ebooks.iospress.nl/doi/10.3233/SHTI210939">https://ebooks.iospress.nl/doi/10.3233/SHTI210939&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Usability Study (2022):&lt;/strong>&lt;br>
&lt;a href="https://ebooks.iospress.nl/doi/10.3233/SHTI220653">https://ebooks.iospress.nl/doi/10.3233/SHTI220653&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>GottBERT</title><link>/project/gottbert/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>/project/gottbert/</guid><description>&lt;h2 id="a-journey-marked-by-endurance-transition-and-discovery">A Journey Marked by Endurance, Transition, and Discovery&lt;/h2>
&lt;p>GottBERT started from a simple observation: while BERT-like models were rapidly advancing NLP research and RoBERTa had become a widely adopted standard, there was no single-language RoBERTa model for German. Multilingual models were everywhere, but consistently fell short of high-quality monolingual models in a given language.&lt;/p>
&lt;p>This gap was the starting point for GottBERT.&lt;/p>
&lt;p>The first version emerged before my move to Munich, a project driven by scientific curiosity that quickly gained momentum. Trained on a 256-core TPUv3 Pod using the German portion of the OSCAR corpus, the initial GottBERT model followed the original RoBERTa pre-training recipe implemented in fairseq. Even without elaborate hyperparameter tuning, it delivered strong results: across multiple NER and text-classification benchmarks, GottBERT outperformed many tested German and multilingual comparison models.&lt;/p>
&lt;p>With the relocation to Munich, the second phase of the project began. GottBERT was further refined, more deeply evaluated, and extended with a Large variant. During this phase, we also examined the impact of filtering the OSCAR corpus, a reasonable hypothesis that turned out differently than expected. Despite corpus filtering, performance remained largely unchanged.&lt;/p>
&lt;p>In the updated EMNLP version, the focus shifted from reporting the initial results to providing a more grounded scientific framing, including a clearer comparison with existing German and multilingual BERT models, an extended evaluation across downstream tasks, and an investigation of how filtering the OSCAR corpus affects pre-training quality.&lt;/p>
&lt;p>Today, GottBERT stands as the first German RoBERTa model, freely available to the research community, originally under AGPLv3, later relicensed under MIT. It remains one of the projects that shaped my research journey: the combination of a clear gap, technical ambition, a geographical transition, academic maturation, and a final result that meaningfully supports the German NLP community.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2020):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2012.02110v1">https://arxiv.org/abs/2012.02110v1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>EMNLP 2024 Paper:&lt;/strong>&lt;br>
&lt;a href="https://aclanthology.org/2024.emnlp-main.1183/">https://aclanthology.org/2024.emnlp-main.1183/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Poster &amp;amp; Presentation (Underline):&lt;/strong>&lt;br>
&lt;a href="https://underline.io/events/469/posters/18793/poster/108142-gottbert-a-pure-german-language-model?tab=abstract">https://underline.io/events/469/posters/18793/poster/108142-gottbert-a-pure-german-language-model?tab=abstract&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/TUM">https://huggingface.co/TUM&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>MIRACUM-Pipe</title><link>/project/miracum-pipeline/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/project/miracum-pipeline/</guid><description>&lt;h2 id="miracum-pipe--standardizing-ngs-analysis-for-precision-oncology">MIRACUM-Pipe â€“ Standardizing NGS Analysis for Precision Oncology&lt;/h2>
&lt;p>MIRACUM-Pipe was one of my main technical contributions during 2019 and 2020, in the context of the MIRACUM consortium and its precision oncology workflows. The pipeline itself had already been established, but required significant modernization, restructuring, and containerization to become robust, reproducible, and easy to deploy across the various MIRACUM sites.&lt;/p>
&lt;p>My first contribution was a deep refactoring of the existing workflow, which at the time consisted of a set of Bash scripts that had grown organically. I restructured the codebase, cleaned up the scripting logic, modularized core components, and ensured a more maintainable and transparent execution flow. This laid the foundation for the next step: a fully dockerized version of the pipeline that could be executed in a standardized, reproducible way independent of local environments.&lt;/p>
&lt;p>To achieve this, a second project was created to expose the pipeline via Docker, making installation and usage significantly easier for newly onboarding MIRACUM centers and enabling reproducible runs across different infrastructures. The dockerized version soon became the recommended deployment approach. The entire code was published publicly under the AGPLv3 license.&lt;/p>
&lt;p>An early attempt was made to publish the pipeline as an Application Note in Bioinformatics, but this submission was not accepted. Later, after several subsequent extensions and improvements, completed when I was no longer actively involved, the pipeline was successfully published in MDPI Cancers. The publication reflects the combined efforts of the team and documents MIRACUM-Pipe as a comprehensive NGS analysis solution.&lt;/p>
&lt;p>MIRACUM-Pipe supports the complete analysis workflow needed in precision oncology and Molecular Tumor Boards (MTBs). It provides a one-prompt solution covering quality control, variant calling, copy number estimation, functional annotation, visualization, and automated report generation. By standardizing these steps, the pipeline supports MTB case preparation and presentation by generating consistent, high-quality sequencing summaries for clinicians and researchers.&lt;/p>
&lt;p>The associated repositories remain available on GitHub, including both the primary codebase and the dockerized variant, and continue to be used within and beyond the MIRACUM consortium.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Publication (Cancers, 2023):&lt;/strong>&lt;br>
&lt;a href="https://www.mdpi.com/2072-6694/15/13/3456">https://www.mdpi.com/2072-6694/15/13/3456&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitHub Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/AG-Boerries/MIRACUM-Pipe">https://github.com/AG-Boerries/MIRACUM-Pipe&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Dockerized Version:&lt;/strong>&lt;br>
&lt;a href="https://github.com/AG-Boerries/MIRACUM-Pipe-docker">https://github.com/AG-Boerries/MIRACUM-Pipe-docker&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>tala-med Search Engine</title><link>/project/tala-med/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/project/tala-med/</guid><description>&lt;h2 id="tala-med-search--engineering-a-medical-search-engine-from-prototype-to-modern-ir-platform">tala-med search â€“ Engineering a Medical Search Engine from Prototype to Modern IR Platform&lt;/h2>
&lt;p>The tala-med search engine began in 2019 as a GAP funded project. I joined initially as technical support, but after a colleague left the team I took over the development lead and guided the system through several difficult phases into a working prototype that could be evaluated scientifically. The early version of tala-med used a React interface based on design drafts created by a graphic designer in the group. On the backend, we used AppSearch together with the Fess crawler. This setup was sufficient for the first prototype and for the first evaluation, but it became clear that the closed ecosystem limited our flexibility, especially regarding custom NLP components and extensible retrieval pipelines.&lt;/p>
&lt;p>With this in mind we designed a new technology platform that offered full control over indexing, querying, scoring and synonym handling. The architecture was inspired by the experience gained with the MeSH-Browser and built on top of the subZero stack. As a student project we developed a synonym expansion component using a FastText based approach, and even managed to outperform the previous solution in speed. This work resulted in a journal publication documenting the new retrieval platform and its technical design.&lt;/p>
&lt;p>A second major building block was the crawler. The original Fess setup lacked the flexibility and transparency needed for large scale, domain controlled crawling. We migrated to a custom adapted version of Apache Nutch, which became the basis of a student project that later resulted in a peer reviewed MIE 2025 publication. Together, the new retrieval platform and the improved crawler formed a modern, maintainable and fully open architecture for tala-med.&lt;/p>
&lt;p>Both contributions were completed during the earlier development phase. I have since continued to work on tala-med, and we are currently preparing the deployment of the new platform to updated servers. The goal is to replace the old prototype implementation in late 2025 or early 2026 with the fully redesigned architecture.&lt;/p>
&lt;p>tala-med is now a fully re-engineered medical search engine with an open, extensible infrastructure, modern crawling, synonym expansion, and a retrieval pipeline that can be adapted to new NLP methods as needed. What started as a constrained prototype has now matured into a flexible research and production environment backed by multiple publications and several years of iterative development.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Live Search Instance:&lt;/strong>&lt;br>
&lt;a href="https://suche.tala-med.info">https://suche.tala-med.info&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Source Code (Search Platform Repository):&lt;/strong>&lt;br>
&lt;a href="https://gitlab.com/tala-med/search-platform">https://gitlab.com/tala-med/search-platform&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prototype and Human Factors Evaluation (JMIR Human Factors, 2025):&lt;/strong>&lt;br>
&lt;a href="https://humanfactors.jmir.org/2025/1/e56941/">https://humanfactors.jmir.org/2025/1/e56941/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Technical Improvement and Synonym Expansion (Health Informatics Journal, 2025):&lt;/strong>&lt;br>
&lt;a href="https://journals.sagepub.com/doi/full/10.1177/14604582251381271">https://journals.sagepub.com/doi/full/10.1177/14604582251381271&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Custom Apache Nutch Crawler (MIE 2025):&lt;/strong>&lt;br>
&lt;a href="https://ebooks.iospress.nl/doi/10.3233/SHTI250423">https://ebooks.iospress.nl/doi/10.3233/SHTI250423&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>ESID Registry</title><link>/project/esid/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/project/esid/</guid><description>&lt;p>ESID was one of the most formative long-term projects in my early medical informatics career. Through the ESID Registry I had the opportunity to work at the intersection of clinical data quality, registry design, modern database engineering, and distributed reporting systems. It was also the first project where I was able to present work at international conferences early on, and where I could demonstrate empirically that plausibility checks directly lead to higher data quality â€” an insight that later influenced several follow-up systems.&lt;/p>
&lt;p>During the early phase of the redesigned ESID registry, the system itself had already been implemented by the core development group. My role focused on publishing the new registry through an Application Note and presenting its concepts and early insights at ESID meetings. In addition, I developed several of the technical components that became central to the operational ecosystem around the registry, most notably the reporting tool, various export mechanisms (including automated email-based exports), and supporting infrastructure for data access and data delivery. Through these contributions I helped make the registry more accessible, more analyzable, and more useful for both clinicians and researchers.&lt;/p>
&lt;p>A major part of my ESID work centered on the reporting and export infrastructure surrounding the registry. While the registry itself was already in place, its operational ecosystem required modernisation. I developed the reporting tool, which unified several existing data flows, including the integration of UKPID registry exports via custom software. As part of this effort, the underlying database was periodically migrated from MySQL to PostgreSQL in an automated pipeline, enabling more robust data handling and laying the foundation for secure, structured access mechanisms such as Row-Level Security. This environment later supported API-based data retrieval, export interfaces, and interactive reporting functions for both internal and external stakeholders. The pipeline was later stabilized and re-implemented as Snakemake workflow.&lt;/p>
&lt;p>With PostgreSQL in place, we implemented a modern API using the SubZero stack, enabling safe and structured data access. Row-Level Security (RLS) policies were deployed across all tables to guarantee user-specific and registry-specific permissions. In the internal area of the platform, permitted users could generate exports, including specialized outputs such as the APDS study dataset. The system was built around RabbitMQ and Socket-based notifications. The frontend was developed in React and consumed a GraphQL interface built on top of the API layer. The public-facing side provided global registry statistics, while the internal dashboard showed the subset of data relevant to the logged-in userâ€™s registry assignment.&lt;/p>
&lt;p>Later, there were plans to migrate the entire reporting solution to Apache Superset. Initial groundwork was completed: a connection to the user directory was implemented, and the database was preconfigured with RLS so that dashboards would automatically respect each userâ€™s data access permissions. However, due to limited resources and the end of the contract period, the full migration was ultimately not carried out.&lt;/p>
&lt;p>The APDS study was an important use case within the ESID ecosystem. Its dataset required a human-readable export format despite being based on nested data structures. To make these structures visually intuitive, we created a color-coded Excel representation that allowed clinicians to interpret hierarchical data at a glance within a so called sparse spreadsheet. The tool was named Json2Xlsx and was later published in Software Impacts. This export mechanism supported numerous APDS publications and was also used for data exports to industry partners.&lt;/p>
&lt;p>A further component connected to the ESID ecosystem involved the integration of ESID within the German PID-NET infrastructure using the OSSE registry framework. This was documented in a dedicated publication describing how the OSSE bridgehead enables interoperability between the customized ESID registry and the wider network of rare disease registries. While my own work focused primarily on reporting, export mechanisms, API access, and data delivery around ESID, this paper provides the architectural context in which our technical ecosystem operated: it shows how the ESID registry can participate in decentralized, federated queries without relinquishing data sovereignty, and how OSSE acts as an interoperability layer for national and international collaborative research. The approach demonstrated how a highly customized registry like ESID can connect to the OSSE platform via a free and maintainable toolchain, illustrating the broader interoperability landscape in which my ESID-related technical contributions were embedded.&lt;/p>
&lt;p>Overall, ESID represents a central pillar of my early work in registry design, data quality, API-driven architectures, and secure access control via PostgreSQL and RLS. It is one of the projects where methodological development, practical clinical utility, and research contributions came together in a meaningful way, and one that influenced how I approached subsequent registry and data pipeline projects.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>New ESID Registry â€“ Application Note:&lt;/strong>&lt;br>
&lt;a href="https://academic.oup.com/bioinformatics/article/35/24/5367/5526873">https://academic.oup.com/bioinformatics/article/35/24/5367/5526873&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Reporting Tool:&lt;/strong>&lt;br>
&lt;a href="https://cci-reporting.uniklinik-freiburg.de/#/">https://cci-reporting.uniklinik-freiburg.de/#/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Registry Overview (2020, Frontiers in Immunology):&lt;/strong>&lt;br>
&lt;a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7578818/">https://pmc.ncbi.nlm.nih.gov/articles/PMC7578818/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Registry Working Party:&lt;/strong>&lt;br>
&lt;a href="https://esid.org/working-parties/registry-working-party/">https://esid.org/working-parties/registry-working-party/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>AL-PID Registry</title><link>/project/al-pid/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>/project/al-pid/</guid><description>&lt;h2 id="al-pid--from-a-legacy-access-structure-to-a-functional-clinical-registry">AL-PID â€“ From a Legacy Access Structure to a Functional Clinical Registry&lt;/h2>
&lt;p>AL-PID was the very first project I worked on when I joined the CCI in 2015. What existed initially was a raw Microsoft Access database intended to serve as a registry for patients with ALPS and related primary immunodeficiencies. What was missing was everything required for actual clinical use: a user interface, a coherent data model, plausibility logic, and an export workflow suitable for downstream scientific analysis.&lt;/p>
&lt;p>The goal was therefore clear: transform a static legacy database into a usable, validated clinical registry.&lt;/p>
&lt;p>I developed a complete user interface in Microsoft Access and Visual Basic, inspired by ESID but built independently. Central to the system was a tree-structured plausibility logic implemented through event-driven hooks such as OnClick and OnChange. This logic ensured consistent, clinically meaningful data entry and protected the dataset from contradictory or incomplete inputs.&lt;/p>
&lt;p>A distinctive aspect of AL-PID was its dynamic, visually guided plausibility system. The UI did not simply validate data in the background, but actively controlled the userâ€™s workflow by showing or hiding interface elements based on clinical context. Only when certain variables were selected did additional fields, sections, or action buttons appear. This approach allowed clinicians to focus on the exact data relevant to the case at hand, reduced cognitive load, and prevented invalid combinations from ever being entered. It was a lightweight but highly effective form of context-aware data entry entirely implemented within Access and VBA.&lt;/p>
&lt;p>Over time, AL-PID evolved not only at the UI level but also structurally. The original data model had grown organically and lacked clear boundaries between clinical entities. I refactored and abstracted the schema: redundant fields were consolidated, variable groups reorganized, and a clean separation between MDAT and IDAT was introduced to satisfy data protection requirements. The result was a modular and extensible data model that improved maintainability, validation, and long-term data quality. This evolving structure became the backbone that enabled stable exports for subsequent statistical workflows.&lt;/p>
&lt;p>During my later guest scientist period at the CCI, clinicians required a publication-ready export. We designed an Excel-based format that medical staff could populate without technical hurdles. I then implemented a Python-based exporter that converted the sheets into a standardized dataset suitable for further analysis. This workflow ultimately enabled a successful publication in The Lancet Haematology, marking both the scientific impact of the project and the maturity of the registry system.&lt;/p>
&lt;p>Looking back, AL-PID was a defining early project for me: a demanding but formative introduction to clinical registries, data modeling, and the practical realities of medical informatics. Many of the principles developed here later influenced my subsequent registry and data pipeline work, including ESID and OSSE-based systems.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Project Overview (CCI Freiburg):&lt;/strong>&lt;br>
&lt;a href="https://www.uniklinik-freiburg.de/cci/studien/alpsal-pid.html">https://www.uniklinik-freiburg.de/cci/studien/alpsal-pid.html&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>Publication (The Lancet Haematology):&lt;/strong>&lt;br>
&lt;a href="https://www.thelancet.com/journals/lanhae/article/PIIS2352-3026(23)00362-9">https://www.thelancet.com/journals/lanhae/article/PIIS2352-3026(23)00362-9&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>OctoMap Contribution</title><link>/project/octomap/</link><pubDate>Mon, 20 Dec 2010 00:00:00 +0000</pubDate><guid>/project/octomap/</guid><description>&lt;h2 id="efficient-octree-traversal-for-robot-navigation-with-octomap">Efficient octree traversal for robot navigation with OctoMap&lt;/h2>
&lt;p>My first larger C++ project began in 2010 when I wrote my bachelor thesis in the context of OctoMap, the well known open source framework for probabilistic 3D mapping. The project marked my first real contact with C++ and introduced me to volumetric data structures, robotics concepts, and efficient tree based algorithms.&lt;/p>
&lt;p>The goal of the thesis was to investigate and accelerate octree traversal methods for ray based operations in 3D volumetric maps. Autonomous robots rely on probabilistic 3D models for collision free navigation and path planning, and OctoMap provides such models by storing occupancy information in a compressed octree structure. Insertions and updates are driven by raycasting and raytracing, which in turn depend heavily on traversal performance.&lt;/p>
&lt;p>The work explored multiple traversal strategies within OctoMap and evaluated how shortcuts, reorganized traversal orderings, and memory handling affect the speed of ray based operations. The experiments demonstrated that raytracing can be accelerated through specific traversal shortcuts, while raycasting benefits significantly from improved memory management strategies. These optimizations showed measurable gains in the insertion and update cycle of 3D sensor data.&lt;/p>
&lt;p>Building on these findings, the thesis also introduced an approach for segmenting OctoMaps into connected components of free or occupied space. Two segmentation variants were implemented and analyzed, one optimized for the compressed octree representation, and one for the expanded representation. The resulting algorithms were statistically evaluated and demonstrated how connected components could form a foundation for later object recognition or higher level map analysis.&lt;/p>
&lt;p>This project was my entry point into robotics oriented software engineering and 3D data structures, and it remains a meaningful milestone from the early days of my academic path.&lt;/p>
&lt;hr>
&lt;h2 id="links-and-resources">Links and Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>OctoMap Project Homepage:&lt;/strong>&lt;br>
&lt;a href="https://octomap.github.io/">https://octomap.github.io/&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Porzellanland Schmitt GmbH</title><link>/project/porzellanland/</link><pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate><guid>/project/porzellanland/</guid><description>&lt;h2 id="porzellanland-schmitt-gmbh--a-family-business-shaped-by-early-digital-entrepreneurship">Porzellanland Schmitt GmbH â€“ A Family Business Shaped by Early Digital Entrepreneurship&lt;/h2>
&lt;p>My involvement with Porzellanland began long before my academic career. As a highschool student, I accompanied my parents when they started a porcelain trading business. The seed for the business was planted in a simple everyday situation: my grandmother needed a single replacement cup from a discontinued porcelain series. My mother complained that sellers only offered complete sets. I spontaneously suggested, &amp;ldquo;Then buy the whole set, gift the parts you need, and sell the rest with profit.&amp;rdquo; That idea became the initial spark for what later grew into a full busieness.&lt;/p>
&lt;p>Very early, I became responsible for the technical backbone of the business. I set up the first online shop using osCommerce, including server installation, configuration, and the complete infrastructure needed for a functioning e-commerce platform. The limitations of osCommerce, combined with the flexibility of its open source structure and documentation, motivated me to build a fully custom shop system as part of my Abitur project. This system ran successfully for several years and formed the operational foundation of the business during its early growth.&lt;/p>
&lt;p>During my master&amp;rsquo;s studies, I migrated the shop to Magento Open Source and developed custom modules, including region-based shipping calculations and a LaTeX based PDF invoice generator. Later, the business transitioned to Shopify, where the shop continued to evolve without my direct involvement.&lt;/p>
&lt;p>Beyond the technical aspects, I also contributed to shaping the brand identity. The name Porzellanland, the domain, and the original logo were all created in collaboration with me. Over the years, the company became one of the largest specialized dealers for discontinued branded porcelain, shipping internationally and serving a niche market with high reliability.&lt;/p>
&lt;p>Today, the business is in its final chapter and planned to be dissolved at the end of the year 2025. Yet Porzellanland remains an important part of my personal and entrepreneurial story, a place where I learned early how technology, design, and business can work together, long before I entered the academic world.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Porzellanland Website:&lt;/strong>&lt;br>
&lt;a href="https://porzellanland.de">https://porzellanland.de&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>