<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Efficiency on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</title><link>/tags/efficiency/</link><description>Recent content in Efficiency on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year}</copyright><lastBuildDate>Fri, 24 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/efficiency/index.xml" rel="self" type="application/rss+xml"/><item><title>SindBERT</title><link>/project/sindbert/</link><pubDate>Fri, 24 Oct 2025 00:00:00 +0000</pubDate><guid>/project/sindbert/</guid><description>&lt;h2 id="sindbert--the-turkish-model-that-started-a-new-chapter">SindBert â€“ The Turkish Model That Started a New Chapter&lt;/h2>
&lt;p>SindBert started as an exploratory attempt to bring high-quality RoBERTa-style language modeling to Turkish. The idea originally emerged during the broader multilingual phase that followed GottBERT, GeistBERT, ChristBERT, HalleluBERT and PortBERT. After finishing PortBERT, the timing finally felt right to extend this line of research to another underrepresented language.&lt;/p>
&lt;p>With the pipeline already stable from previous projects, we created the first large-scale RoBERTa-style Turkish model. This makes SindBert a significant technical milestone within the landscape of Turkish transformer models, and an important step toward raising Turkish NLP to the architectural standards used in higher-resource languages.&lt;/p>
&lt;p>The study behind SindBert was conducted with considerable care. We systematically reviewed existing Turkish models and evaluated them across an extensive set of downstream benchmarks. The evaluation was carried out in two stages: first on a private workstation with two RTX 3090 GPUs in SLI, and later on the LRZ BayernKI H100 cluster, which enabled the training and assessment of the large SindBert variants.&lt;/p>
&lt;p>A key insight from the study was that the performance of Turkish models is not driven simply by corpus size. Extremely large corpora did not consistently translate into stronger results. Instead, the best performance came from models trained on corpora with moderate size but high internal variance and quality. This finding reinforces the idea that thoughtful corpus design often matters more than sheer quantity for languages with complex morphology.&lt;/p>
&lt;p>Although SindBERT does not yet have a formal publication attached to it, the project marks the beginning of a new chapter in my international research. It follows directly after PortBERT and represents the point where the broader model family began to expand into new linguistic territory. The work was completed in Freiburg at my new research institution, where the evaluations and analyses were finalized. SindBERT may evolve further or be revisited in a second iteration, but even in its current form it stands as the first large Turkish RoBERTa-style model and an important step toward stronger transformer development for the Turkish language. A preprint of the work is available, and the paper has been submitted to SIGTURK@EACL 2026.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2510.21364">https://arxiv.org/abs/2510.21364&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/SindBERT">https://huggingface.co/SindBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>PortBERT</title><link>/project/portbert/</link><pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate><guid>/project/portbert/</guid><description>&lt;h2 id="portbert--exploring-efficiency-under-pressure-and-in-deep-waters">PortBERT â€“ Exploring Efficiency Under Pressure and in Deep Waters&lt;/h2>
&lt;p>PortBERT has its roots in an early collaboration idea from 2021/22, when a Brazilian research group approached me with the vision of building a Portuguese RoBERTa-style model. I shared my pre-training pipeline with them, but at the time, the fairseq RoBERTa branch had a broken implementation and my preprocessing workflow contained an unnoticed bug. As a result, the collaboration could not produce a functional model, and the project quietly came to a halt.&lt;/p>
&lt;p>Years later, during a research stay on the Azores, I revisited the idea independently. The full Portuguese pre-training had actually been carried out earlier, during the GottBERT development phase, when we debugged and stabilized the pipeline. With the system running reliably at that time, I launched a complete Portuguese pre-training run on a TPUv4-128 pod and on a GPU server, resulting in the PortBERT base and large models.&lt;/p>
&lt;p>The period on the Azores added a unique dimension to the project.&lt;br>
It was a time of intense pressure: I was waiting for the contract from my upcoming research position in Freiburg, while simultaneously completing my divemaster internship and training. Much of the evaluation work was carried out between deep dives, decompression lessons, and long days of practical training. This phase taught me, quite literally, to breathe under pressure. In hindsight, PortBERT became the model that was built and evaluated in the rhythm of the ocean: long, calm stretches of computation framed by physical depth and mental discipline.&lt;/p>
&lt;p>The evaluation revealed an important insight:&lt;br>
efficiency is an essential dimension of model design.&lt;br>
PortBERT achieved competitive results despite being smaller and more cost-efficient than many recent Portuguese encoder LLMs. The study also highlighted that the Portuguese corpus could benefit from greater internal variance, and that techniques like Whole Word Masking might offer additional improvements, though WWM was not available within the TPU workflow used here.&lt;/p>
&lt;p>With this, PortBERT introduced a new perspective into the Portuguese NLP landscape:&lt;br>
that efficiency, compute cost, corpus composition, and training dynamics should receive as much attention as raw performance. The goal of the project was not only to build another model, but to encourage the community to look beyond leaderboard scores when developing modern transformer architectures and models.&lt;/p>
&lt;h2 id="portbert-was-presented-at-globalnlp2025ranlp-2025httpsglobalnlp2025githubio-in-varna-bulgaria-and-we-are-currently-awaiting-its-release-on-the-acl-anthology-the-model-itself-is-publicly-available-on-huggingface">PortBERT was presented at &lt;a href="https://globalnlp2025.github.io">GlobalNLP2025@RANLP 2025&lt;/a> in Varna, Bulgaria, and we are currently awaiting its release on the ACL Anthology. The model itself is publicly available on HuggingFace.&lt;/h2>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/PortBERT">https://huggingface.co/PortBERT&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>