<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Featured on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</title><link>/tags/featured/</link><description>Recent content in Featured on Raphael Schmitt â€” AI Researcher | Language Models, NLP &amp; Information Retrieval</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year}</copyright><lastBuildDate>Thu, 18 Sep 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/featured/index.xml" rel="self" type="application/rss+xml"/><item><title>ChristBERT</title><link>/project/christbert/</link><pubDate>Thu, 18 Sep 2025 00:00:00 +0000</pubDate><guid>/project/christbert/</guid><description>&lt;h2 id="building-a-medical-german-language-model-against-the-odds">Building a Medical German Language Model Against the Odds&lt;/h2>
&lt;p>ChristBERT was developed as part of my Masterâ€™s thesis and focuses on domain-specific German clinical language modeling. The project investigated two complementary strategies: continued pre-training on top of GottBERT/GeistBERT, and training from scratch using both a general-purpose RoBERTa tokenizer and a specialized medical vocabulary. This enabled a systematic comparison between general, domain-adapted, and fully specialized pre-training pipelines for German medical NLP.&lt;/p>
&lt;p>During the early conceptual phase, the supervision structure of the thesis changed, and the project was reassigned to a new advisor, who later became the supervisor of my doctoral work. ChristBERT was trained using a combination of compute resources: through an existing project partnership, parts of the full pre-training were executed on the Augsburg compute cluster, while downstream evaluations, and parts of the translation workflows ran on privately built GPU hardware in my basement.&lt;/p>
&lt;p>ChristBERT showed that continued pre-training on German clinical text offers measurable improvements over general-purpose RoBERTa models, while training from scratch with a medical-specific tokenizer provides additional insights into domain vocabulary specialization and its trade-offs.&lt;/p>
&lt;p>The work has been released as a preprint and is currently under review at BMC Medical Informatics and Decision Making.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2025):&lt;/strong>&lt;br>
&lt;a href="https://www.researchsquare.com/article/rs-7332811/v1">https://www.researchsquare.com/article/rs-7332811/v1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/ChristBERT">https://huggingface.co/ChristBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Volume Renderer for use with MATLAB</title><link>/project/volume-renderer/</link><pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate><guid>/project/volume-renderer/</guid><description>&lt;h2 id="from-a-student-project-to-a-polished-tool-gpu-volume-rendering-for-matlab">From a student project to a polished tool: GPU volume rendering for MATLAB&lt;/h2>
&lt;p>Volume Renderer for use with MATLAB started as my master project at the University of Freiburg in 2012, supervised by Benjamin Ummenhofer and apl. Prof. Dr. Olaf Ronneberger. At the time, MATLAB offered only limited support for interactive, high quality volume rendering of 3D data on the GPU, especially for larger medical volumes and multi volume scenes. The project filled this gap by adding a GPU enabled volume render command to MATLAB that could be used directly from scripts and applications.&lt;/p>
&lt;p>The core renderer is written in CUDA C and C++, while the user interface is provided through a set of MATLAB classes. This design separates performance critical code from user facing logic and allowed us to integrate advanced rendering concepts into a familiar MATLAB workflow. Users can call the renderer like any other MATLAB function, yet the heavy lifting happens on the GPU.&lt;/p>
&lt;p>A central feature of the system is its custom memory management. GPU memory is limited, but many use cases require rendering multiple large volumes in a single scene. To address this, the renderer splits the scene into separate rendering passes that fit into GPU memory, then combines the resulting images inside MATLAB into a single final frame. On top of that, volumes are only transferred to the GPU when the underlying data changes, which keeps repeated renders fast by reusing GPU memory across calls.&lt;/p>
&lt;p>The renderer also implements a generic illumination model that can be extended with different phase functions. The provided implementation uses the Henyey Greenstein phase function, which allows realistic lighting effects for volumetric data. For specific applications the tool supports off axis stereo rendering, which makes it possible to create stereo pairs and 3D movies for immersive visualization.&lt;/p>
&lt;p>High usability was a design goal from the beginning. The MATLAB interface is built around a small set of classes that make it straightforward to configure scenes, adjust parameters, and generate animations programmatically. This makes the renderer suitable not only for research prototypes, but also for teaching and reproducible figure generation.&lt;/p>
&lt;p>After an initial active phase during my studies the project sat idle for several years. When I moved to Munich in 2021 I resumed work on the codebase, updated the toolchain, improved the documentation, and prepared the project for a proper open source release. The result was a journal publication in 2024 and a cleaned up public repository that others can build on.&lt;/p>
&lt;p>Volume Renderer for use with MATLAB is released under the GNU Affero General Public License version 3. The example scripts are licensed under MIT, so they can be easily reused and adapted. The full source code is available on GitHub, together with usage examples and build instructions.&lt;/p>
&lt;hr>
&lt;h2 id="links-and-resources">Links and Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Journal Publication (2024):&lt;/strong>&lt;br>
&lt;a href="https://www.mdpi.com/2673-6470/4/4/49">https://www.mdpi.com/2673-6470/4/4/49&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitHub Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/volume_renderer">https://github.com/raphiniert-com/volume_renderer&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>React Admin Data Providers</title><link>/project/ra-data/</link><pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate><guid>/project/ra-data/</guid><description>&lt;h2 id="react-admin-data-providers-for-clinical-apis--postgrest-and-fhir">React Admin Data Providers for Clinical APIs â€“ PostgREST and FHIR&lt;/h2>
&lt;p>The development of my React Admin data providers began with the MeSH Browser project. At that time, I needed a flexible way to connect a React Admin frontend to a PostgREST backend without manually wiring every query, filter and pagination step. Early community work provided some inspiration, but no complete implementation existed. This led to the first version of the PostgREST data provider, which gradually grew into a fully featured library which is widely used (&amp;gt;2k downloads/week).&lt;/p>
&lt;p>Over time, the project evolved far beyond its original prototype. We added a dedicated test framework, introduced structured configuration, and turned it into a generic ecosystem that makes it easy to build React Admin frontends on top of Postgres. A demo setup was created to showcase multiple PostgreSQL FDWs together with PostgREST, illustrating how flexible the architecture can be when connecting diverse data sources. For authentication and authorization, the system integrates cleanly with Keycloak.&lt;/p>
&lt;p>The second data provider emerged in a similar way. As part of a student project, we built a FHIR REST data provider for React Admin. Its goal was to simplify building clinical user interfaces directly on top of FHIR servers. The data provider implements FHIR search, pagination, resource handling and bundle interpretation, and was tested against the LinuxForHealth (formerly IBM) FHIR Server. Together with a small demo application, this demonstrated how React Admin can be used as a lightweight tool for building FHIR based administrative interfaces. The project resulted in a peer reviewed conference publication at ICIMTH.&lt;/p>
&lt;p>Together, these two libraries form a small but valuable toolkit that connects modern React based user interfaces with established clinical APIs. Whether through PostgREST or FHIR, both data providers lower the barrier for creating custom user interfaces in clinical or research environments by offering reliable, clean and reusable integrations that work out of the box.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;h3 id="postgrest-data-provider">PostgREST Data Provider&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Journal Publication (Software Impacts, 2024):&lt;/strong>&lt;br>
&lt;a href="https://www.sciencedirect.com/science/article/pii/S2665963824000873">https://www.sciencedirect.com/science/article/pii/S2665963824000873&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NPM Package:&lt;/strong>&lt;br>
&lt;a href="https://www.npmjs.com/package/@raphiniert/ra-data-postgrest">https://www.npmjs.com/package/@raphiniert/ra-data-postgrest&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitHub Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/ra-data-postgrest">https://github.com/raphiniert-com/ra-data-postgrest&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Demo Repository:&lt;/strong>&lt;br>
&lt;a href="https://github.com/raphiniert-com/ra-data-postgrest-demo">https://github.com/raphiniert-com/ra-data-postgrest-demo&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h3 id="fhir-data-provider">FHIR Data Provider&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Conference Publication (ICIMTH 2023):&lt;/strong>&lt;br>
&lt;a href="https://doi.org/10.3233/SHTI230436">https://doi.org/10.3233/SHTI230436&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>GitLab Repository:&lt;/strong>&lt;br>
&lt;a href="https://gitlab.com/mri-tum/aiim/libs/ra-data-fhir">https://gitlab.com/mri-tum/aiim/libs/ra-data-fhir&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>NPM Package:&lt;/strong>&lt;br>
&lt;a href="https://www.npmjs.com/package/@tum-mri-aiim/ra-data-fhir">https://www.npmjs.com/package/@tum-mri-aiim/ra-data-fhir&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>GottBERT</title><link>/project/gottbert/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>/project/gottbert/</guid><description>&lt;h2 id="a-journey-marked-by-endurance-transition-and-discovery">A Journey Marked by Endurance, Transition, and Discovery&lt;/h2>
&lt;p>GottBERT started from a simple observation: while BERT-like models were rapidly advancing NLP research and RoBERTa had become a widely adopted standard, there was no single-language RoBERTa model for German. Multilingual models were everywhere, but consistently fell short of high-quality monolingual models in a given language.&lt;/p>
&lt;p>This gap was the starting point for GottBERT.&lt;/p>
&lt;p>The first version emerged before my move to Munich, a project driven by scientific curiosity that quickly gained momentum. Trained on a 256-core TPUv3 Pod using the German portion of the OSCAR corpus, the initial GottBERT model followed the original RoBERTa pre-training recipe implemented in fairseq. Even without elaborate hyperparameter tuning, it delivered strong results: across multiple NER and text-classification benchmarks, GottBERT outperformed many tested German and multilingual comparison models.&lt;/p>
&lt;p>With the relocation to Munich, the second phase of the project began. GottBERT was further refined, more deeply evaluated, and extended with a Large variant. During this phase, we also examined the impact of filtering the OSCAR corpus, a reasonable hypothesis that turned out differently than expected. Despite corpus filtering, performance remained largely unchanged.&lt;/p>
&lt;p>In the updated EMNLP version, the focus shifted from reporting the initial results to providing a more grounded scientific framing, including a clearer comparison with existing German and multilingual BERT models, an extended evaluation across downstream tasks, and an investigation of how filtering the OSCAR corpus affects pre-training quality.&lt;/p>
&lt;p>Today, GottBERT stands as the first German RoBERTa model, freely available to the research community, originally under AGPLv3, later relicensed under MIT. It remains one of the projects that shaped my research journey: the combination of a clear gap, technical ambition, a geographical transition, academic maturation, and a final result that meaningfully supports the German NLP community.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Preprint (2020):&lt;/strong>&lt;br>
&lt;a href="https://arxiv.org/abs/2012.02110v1">https://arxiv.org/abs/2012.02110v1&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>EMNLP 2024 Paper:&lt;/strong>&lt;br>
&lt;a href="https://aclanthology.org/2024.emnlp-main.1183/">https://aclanthology.org/2024.emnlp-main.1183/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Poster &amp;amp; Presentation (Underline):&lt;/strong>&lt;br>
&lt;a href="https://underline.io/events/469/posters/18793/poster/108142-gottbert-a-pure-german-language-model?tab=abstract">https://underline.io/events/469/posters/18793/poster/108142-gottbert-a-pure-german-language-model?tab=abstract&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ðŸ¤— &lt;strong>HuggingFace Model Hub:&lt;/strong>&lt;br>
&lt;a href="https://huggingface.co/GottBERT">https://huggingface.co/GottBERT&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>tala-med Search Engine</title><link>/project/tala-med/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>/project/tala-med/</guid><description>&lt;h2 id="tala-med-search--engineering-a-medical-search-engine-from-prototype-to-modern-ir-platform">tala-med search â€“ Engineering a Medical Search Engine from Prototype to Modern IR Platform&lt;/h2>
&lt;p>The tala-med search engine began in 2019 as a GAP funded project. I joined initially as technical support, but after a colleague left the team I took over the development lead and guided the system through several difficult phases into a working prototype that could be evaluated scientifically. The early version of tala-med used a React interface based on design drafts created by a graphic designer in the group. On the backend, we used AppSearch together with the Fess crawler. This setup was sufficient for the first prototype and for the first evaluation, but it became clear that the closed ecosystem limited our flexibility, especially regarding custom NLP components and extensible retrieval pipelines.&lt;/p>
&lt;p>With this in mind we designed a new technology platform that offered full control over indexing, querying, scoring and synonym handling. The architecture was inspired by the experience gained with the MeSH-Browser and built on top of the subZero stack. As a student project we developed a synonym expansion component using a FastText based approach, and even managed to outperform the previous solution in speed. This work resulted in a journal publication documenting the new retrieval platform and its technical design.&lt;/p>
&lt;p>A second major building block was the crawler. The original Fess setup lacked the flexibility and transparency needed for large scale, domain controlled crawling. We migrated to a custom adapted version of Apache Nutch, which became the basis of a student project that later resulted in a peer reviewed MIE 2025 publication. Together, the new retrieval platform and the improved crawler formed a modern, maintainable and fully open architecture for tala-med.&lt;/p>
&lt;p>Both contributions were completed during the earlier development phase. I have since continued to work on tala-med, and we are currently preparing the deployment of the new platform to updated servers. The goal is to replace the old prototype implementation in late 2025 or early 2026 with the fully redesigned architecture.&lt;/p>
&lt;p>tala-med is now a fully re-engineered medical search engine with an open, extensible infrastructure, modern crawling, synonym expansion, and a retrieval pipeline that can be adapted to new NLP methods as needed. What started as a constrained prototype has now matured into a flexible research and production environment backed by multiple publications and several years of iterative development.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Live Search Instance:&lt;/strong>&lt;br>
&lt;a href="https://suche.tala-med.info">https://suche.tala-med.info&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Source Code (Search Platform Repository):&lt;/strong>&lt;br>
&lt;a href="https://gitlab.com/tala-med/search-platform">https://gitlab.com/tala-med/search-platform&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Prototype and Human Factors Evaluation (JMIR Human Factors, 2025):&lt;/strong>&lt;br>
&lt;a href="https://humanfactors.jmir.org/2025/1/e56941/">https://humanfactors.jmir.org/2025/1/e56941/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Technical Improvement and Synonym Expansion (Health Informatics Journal, 2025):&lt;/strong>&lt;br>
&lt;a href="https://journals.sagepub.com/doi/full/10.1177/14604582251381271">https://journals.sagepub.com/doi/full/10.1177/14604582251381271&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Custom Apache Nutch Crawler (MIE 2025):&lt;/strong>&lt;br>
&lt;a href="https://ebooks.iospress.nl/doi/10.3233/SHTI250423">https://ebooks.iospress.nl/doi/10.3233/SHTI250423&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>ESID Registry</title><link>/project/esid/</link><pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate><guid>/project/esid/</guid><description>&lt;p>ESID was one of the most formative long-term projects in my early medical informatics career. Through the ESID Registry I had the opportunity to work at the intersection of clinical data quality, registry design, modern database engineering, and distributed reporting systems. It was also the first project where I was able to present work at international conferences early on, and where I could demonstrate empirically that plausibility checks directly lead to higher data quality â€” an insight that later influenced several follow-up systems.&lt;/p>
&lt;p>During the early phase of the redesigned ESID registry, the system itself had already been implemented by the core development group. My role focused on publishing the new registry through an Application Note and presenting its concepts and early insights at ESID meetings. In addition, I developed several of the technical components that became central to the operational ecosystem around the registry, most notably the reporting tool, various export mechanisms (including automated email-based exports), and supporting infrastructure for data access and data delivery. Through these contributions I helped make the registry more accessible, more analyzable, and more useful for both clinicians and researchers.&lt;/p>
&lt;p>A major part of my ESID work centered on the reporting and export infrastructure surrounding the registry. While the registry itself was already in place, its operational ecosystem required modernisation. I developed the reporting tool, which unified several existing data flows, including the integration of UKPID registry exports via custom software. As part of this effort, the underlying database was periodically migrated from MySQL to PostgreSQL in an automated pipeline, enabling more robust data handling and laying the foundation for secure, structured access mechanisms such as Row-Level Security. This environment later supported API-based data retrieval, export interfaces, and interactive reporting functions for both internal and external stakeholders. The pipeline was later stabilized and re-implemented as Snakemake workflow.&lt;/p>
&lt;p>With PostgreSQL in place, we implemented a modern API using the SubZero stack, enabling safe and structured data access. Row-Level Security (RLS) policies were deployed across all tables to guarantee user-specific and registry-specific permissions. In the internal area of the platform, permitted users could generate exports, including specialized outputs such as the APDS study dataset. The system was built around RabbitMQ and Socket-based notifications. The frontend was developed in React and consumed a GraphQL interface built on top of the API layer. The public-facing side provided global registry statistics, while the internal dashboard showed the subset of data relevant to the logged-in userâ€™s registry assignment.&lt;/p>
&lt;p>Later, there were plans to migrate the entire reporting solution to Apache Superset. Initial groundwork was completed: a connection to the user directory was implemented, and the database was preconfigured with RLS so that dashboards would automatically respect each userâ€™s data access permissions. However, due to limited resources and the end of the contract period, the full migration was ultimately not carried out.&lt;/p>
&lt;p>The APDS study was an important use case within the ESID ecosystem. Its dataset required a human-readable export format despite being based on nested data structures. To make these structures visually intuitive, we created a color-coded Excel representation that allowed clinicians to interpret hierarchical data at a glance within a so called sparse spreadsheet. The tool was named Json2Xlsx and was later published in Software Impacts. This export mechanism supported numerous APDS publications and was also used for data exports to industry partners.&lt;/p>
&lt;p>A further component connected to the ESID ecosystem involved the integration of ESID within the German PID-NET infrastructure using the OSSE registry framework. This was documented in a dedicated publication describing how the OSSE bridgehead enables interoperability between the customized ESID registry and the wider network of rare disease registries. While my own work focused primarily on reporting, export mechanisms, API access, and data delivery around ESID, this paper provides the architectural context in which our technical ecosystem operated: it shows how the ESID registry can participate in decentralized, federated queries without relinquishing data sovereignty, and how OSSE acts as an interoperability layer for national and international collaborative research. The approach demonstrated how a highly customized registry like ESID can connect to the OSSE platform via a free and maintainable toolchain, illustrating the broader interoperability landscape in which my ESID-related technical contributions were embedded.&lt;/p>
&lt;p>Overall, ESID represents a central pillar of my early work in registry design, data quality, API-driven architectures, and secure access control via PostgreSQL and RLS. It is one of the projects where methodological development, practical clinical utility, and research contributions came together in a meaningful way, and one that influenced how I approached subsequent registry and data pipeline projects.&lt;/p>
&lt;hr>
&lt;h2 id="links--resources">Links &amp;amp; Resources&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>New ESID Registry â€“ Application Note:&lt;/strong>&lt;br>
&lt;a href="https://academic.oup.com/bioinformatics/article/35/24/5367/5526873">https://academic.oup.com/bioinformatics/article/35/24/5367/5526873&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Reporting Tool:&lt;/strong>&lt;br>
&lt;a href="https://cci-reporting.uniklinik-freiburg.de/#/">https://cci-reporting.uniklinik-freiburg.de/#/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Registry Overview (2020, Frontiers in Immunology):&lt;/strong>&lt;br>
&lt;a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC7578818/">https://pmc.ncbi.nlm.nih.gov/articles/PMC7578818/&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>ESID Registry Working Party:&lt;/strong>&lt;br>
&lt;a href="https://esid.org/working-parties/registry-working-party/">https://esid.org/working-parties/registry-working-party/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>